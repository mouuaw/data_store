{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbaseconda3e8d1efd23e942ffb39a1c2f774bdc3f",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def get_model_cv_prediction(model, X_data, y_target):\n",
    "    neg_mse_scores = cross_val_score(model, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5)\n",
    "    rmse_scores = np.sqrt(-1 * neg_mse_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    print('##### ', model.__class__.__name__, ' #####')\n",
    "    print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse))\n",
    "\n",
    "def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=False, random_state=0)\n",
    "    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n",
    "    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n",
    "    # print(model.__class__.__name__, ' model 시작 ')\n",
    "\n",
    "    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n",
    "        # print('\\t 폴드 세트: ', folder_counter, ' 시작 ')\n",
    "        X_tr = X_train_n[train_index]\n",
    "        y_tr = y_train_n[train_index]\n",
    "        X_te = X_train_n[valid_index]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1, 1)\n",
    "        test_pred[:, folder_counter] = model.predict(X_test_n)\n",
    "\n",
    "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)\n",
    "\n",
    "    return train_fold_pred, test_pred_mean\n",
    "\n",
    "def rmse_expm1(pred, true):\n",
    "    return -np.sqrt(np.mean((np.expm1(pred)-np.expm1(true))**2))\n",
    "\n",
    "def evaluate(model, x_data, y_data):\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, random_state=0)\n",
    "    model.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=100, verbose=False)\n",
    "    val_pred = model.predict(x_val)\n",
    "    score = rmse_expm1(val_pred, y_val)\n",
    "    return score\n",
    "\n",
    "def rfe(x_data, y_data, method, ratio=0.9, min_feats=40):\n",
    "    feats = x_data.columns.tolist()\n",
    "    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])\n",
    "    while True:\n",
    "        model = LGBMRegressor(objective='regression', num_iterations=10**5)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=0)\n",
    "        model.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=100, verbose=False)\n",
    "        val_pred = model.predict(x_val)\n",
    "        score = rmse_expm1(val_pred, y_val)\n",
    "        n_feats = len(feats)\n",
    "        print(n_feats, score)\n",
    "        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)\n",
    "        if method == 'basic':\n",
    "            feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)\n",
    "        elif method == 'perm':\n",
    "            perm = PermutationImportance(model, random_state=0).fit(x_val, y_val)\n",
    "            feat_imp = pd.Series(perm.feature_importances_, index=feats).sort_values(ascending=False)\n",
    "        elif method == 'shap':\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(x_data[feats])\n",
    "            feat_imp = pd.Series(np.abs(shap_values).mean(axis=0), index=feats).sort_values(ascending=False)\n",
    "        next_n_feats = int(n_feats * ratio)\n",
    "        if next_n_feats < min_feats:\n",
    "            break\n",
    "        else:\n",
    "            feats = feat_imp.iloc[:next_n_feats].index.tolist()\n",
    "    return archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv', index_col='id')\n",
    "test = pd.read_csv('../data/test.csv', index_col='id')\n",
    "submission = pd.read_csv('../data/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMinute = (train.index%144).astype(int)\n",
    "testMinute = (test.index%144).astype(int)\n",
    "\n",
    "train['min'] = trainMinute\n",
    "test['min'] = testMinute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['X00', 'X01', 'X02', 'X03', 'X05', 'X06', 'X07', 'X08', 'X09', 'X10', 'X11', 'X12', 'X13', 'X15', 'X17', 'X18', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X37', 'X38', 'X39', 'min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "xgbr = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.7, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=1, missing=None, n_estimators=1500,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, verbosity=1)\n",
    "\n",
    "lgbmr = LGBMRegressor(objective='regression', num_iterations=10**5)\n",
    "\n",
    "lasso_reg = Lasso(alpha= 0.001)\n",
    "\n",
    "model_list = [xgbr, lgbmr, lasso_reg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[17:58:34] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-1576211398048.5164"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "trainIsNull = train[train['Y18'].isnull()]\n",
    "\n",
    "\n",
    "x_data = trainIsNull[features]\n",
    "y_data = trainIsNull['Y00']\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, random_state=0)\n",
    "\n",
    "xgbr.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=100, verbose=False)\n",
    "\n",
    "val_pred = xgbr.predict(x_val)\n",
    "score = rmse_expm1(val_pred, y_val)\n",
    "score\n",
    "\n",
    "## min 변수가 없으면 -2025780998808.443\n",
    "## 있으면 -1839436923909.5493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perm_cols(model, x_val, y_val, train_df, target_col_name):\n",
    "    features = ['X00', 'X01', 'X02', 'X03', 'X05', 'X06', 'X07', 'X08', 'X09', 'X10', 'X11', 'X12', 'X13', 'X15', 'X17', 'X18', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X37', 'X38', 'X39', 'min']\n",
    "\n",
    "    ## perm 작업을 하기전에 model fit을 수행함\n",
    "    x_data = train_df[features]\n",
    "    y_data = train_df['Y00']\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, random_state=0)\n",
    "    model.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=100, verbose=False)\n",
    "\n",
    "    ## fit 된 model을 permutation 작업수행\n",
    "\n",
    "    perm = PermutationImportance(model, random_state=0).fit(x_val, y_val)\n",
    "    perm_feat_imp = pd.Series(perm.feature_importances_, index=features).sort_values(ascending=False)\n",
    "    return perm_feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_best_col_num(model, x_val, y_val, train_df, target_col_name):\n",
    "    perm_feat_imp = get_perm_cols(model, x_val, y_val, train_df, target_col_name)\n",
    "    perm_cols = []\n",
    "    for i in range(10, 20, 1):\n",
    "        eval_res = evaluate(model, train_df[perm_feat_imp.iloc[:i].index], train_df[target_col_name])\n",
    "        perm_cols.append((i, eval_res))\n",
    "\n",
    "    sorted = pd.DataFrame(perm_cols, columns=['col_num', 'score']).sort_values('score', ascending=False)\n",
    "    print(sorted)\n",
    "    col_num = sorted['col_num'].iloc[0]\n",
    "\n",
    "    return col_num, perm_feat_imp\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    X00    X01  X02  X03  X04     X05    X06   X07     X08     X09  ...  Y10  \\\nid                                                                  ...        \n0   9.7  988.8  1.2  0.6  0.0  1009.3  989.6  12.2  1009.9  1009.8  ...  7.5   \n1   9.3  988.9  1.7  1.9  0.0  1009.3  989.6  12.1  1010.0  1009.9  ...  7.5   \n2   9.4  989.0  1.1  2.3  0.0  1009.2  989.7  12.1  1010.1  1010.1  ...  7.5   \n3   9.4  988.9  1.5  0.7  0.0  1009.2  989.6  12.0  1010.0  1010.0  ...  7.0   \n4   9.2  988.9  0.8  1.7  0.0  1009.2  989.7  12.0  1010.1  1010.0  ...  7.0   \n\n    Y11  Y12   Y13  Y14  Y15  Y16  Y17  Y18  min  \nid                                                \n0   7.0  9.0  10.0  9.5  9.0  8.0  9.0  NaN    0  \n1   7.0  8.5  10.0  9.5  9.0  7.5  9.0  NaN    1  \n2   6.5  8.0   9.5  9.5  8.5  7.5  8.5  NaN    2  \n3   6.0  8.0   9.5  9.0  8.5  7.5  8.5  NaN    3  \n4   6.0  7.5   9.5  9.0  8.5  7.5  8.5  NaN    4  \n\n[5 rows x 60 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X00</th>\n      <th>X01</th>\n      <th>X02</th>\n      <th>X03</th>\n      <th>X04</th>\n      <th>X05</th>\n      <th>X06</th>\n      <th>X07</th>\n      <th>X08</th>\n      <th>X09</th>\n      <th>...</th>\n      <th>Y10</th>\n      <th>Y11</th>\n      <th>Y12</th>\n      <th>Y13</th>\n      <th>Y14</th>\n      <th>Y15</th>\n      <th>Y16</th>\n      <th>Y17</th>\n      <th>Y18</th>\n      <th>min</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9.7</td>\n      <td>988.8</td>\n      <td>1.2</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>1009.3</td>\n      <td>989.6</td>\n      <td>12.2</td>\n      <td>1009.9</td>\n      <td>1009.8</td>\n      <td>...</td>\n      <td>7.5</td>\n      <td>7.0</td>\n      <td>9.0</td>\n      <td>10.0</td>\n      <td>9.5</td>\n      <td>9.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9.3</td>\n      <td>988.9</td>\n      <td>1.7</td>\n      <td>1.9</td>\n      <td>0.0</td>\n      <td>1009.3</td>\n      <td>989.6</td>\n      <td>12.1</td>\n      <td>1010.0</td>\n      <td>1009.9</td>\n      <td>...</td>\n      <td>7.5</td>\n      <td>7.0</td>\n      <td>8.5</td>\n      <td>10.0</td>\n      <td>9.5</td>\n      <td>9.0</td>\n      <td>7.5</td>\n      <td>9.0</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9.4</td>\n      <td>989.0</td>\n      <td>1.1</td>\n      <td>2.3</td>\n      <td>0.0</td>\n      <td>1009.2</td>\n      <td>989.7</td>\n      <td>12.1</td>\n      <td>1010.1</td>\n      <td>1010.1</td>\n      <td>...</td>\n      <td>7.5</td>\n      <td>6.5</td>\n      <td>8.0</td>\n      <td>9.5</td>\n      <td>9.5</td>\n      <td>8.5</td>\n      <td>7.5</td>\n      <td>8.5</td>\n      <td>NaN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9.4</td>\n      <td>988.9</td>\n      <td>1.5</td>\n      <td>0.7</td>\n      <td>0.0</td>\n      <td>1009.2</td>\n      <td>989.6</td>\n      <td>12.0</td>\n      <td>1010.0</td>\n      <td>1010.0</td>\n      <td>...</td>\n      <td>7.0</td>\n      <td>6.0</td>\n      <td>8.0</td>\n      <td>9.5</td>\n      <td>9.0</td>\n      <td>8.5</td>\n      <td>7.5</td>\n      <td>8.5</td>\n      <td>NaN</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9.2</td>\n      <td>988.9</td>\n      <td>0.8</td>\n      <td>1.7</td>\n      <td>0.0</td>\n      <td>1009.2</td>\n      <td>989.7</td>\n      <td>12.0</td>\n      <td>1010.1</td>\n      <td>1010.0</td>\n      <td>...</td>\n      <td>7.0</td>\n      <td>6.0</td>\n      <td>7.5</td>\n      <td>9.5</td>\n      <td>9.0</td>\n      <td>8.5</td>\n      <td>7.5</td>\n      <td>8.5</td>\n      <td>NaN</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 60 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "trainIsNull.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "error.\n[18:36:53] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:36:58] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:37:02] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:37:06] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:37:11] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:37:15] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:37:21] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:37:26] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n   col_num         score\n5       15 -3.925593e+16\n3       13 -4.142074e+16\n6       16 -4.184887e+16\n4       14 -4.435507e+16\n7       17 -4.597893e+16\n9       19 -4.723600e+16\n2       12 -4.775811e+16\n8       18 -4.855887e+16\n0       10 -5.589821e+16\n1       11 -5.809743e+16\nbest col num ::15\n[18:37:31] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\ntarget col ::Y10\n[18:37:37] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:37:55] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:37:59] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:38:02] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:38:06] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:38:11] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:38:15] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:38:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:38:25] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:38:30] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:38:35] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n   col_num         score\n6       16 -1.918828e+19\n5       15 -2.573654e+19\n2       12 -2.741512e+19\n1       11 -2.823848e+19\n9       19 -2.885414e+19\n4       14 -3.077464e+19\n8       18 -3.265026e+19\n0       10 -3.308065e+19\n7       17 -3.777007e+19\n3       13 -4.227569e+19\nbest col num ::16\n[18:38:41] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\ntarget col ::Y11\n[18:38:46] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:39:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:39:08] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:39:12] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:39:16] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:39:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:39:24] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:39:29] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:39:34] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:39:39] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:39:44] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n   col_num         score\n8       18 -2.937788e+21\n9       19 -3.092248e+21\n5       15 -3.128679e+21\n1       11 -3.135752e+21\n7       17 -3.156932e+21\n4       14 -3.191240e+21\n6       16 -3.215333e+21\n0       10 -3.221476e+21\n3       13 -3.245179e+21\n2       12 -3.324821e+21\nbest col num ::18\n[18:39:50] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\ntarget col ::Y12\n[18:39:56] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:40:14] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:40:18] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:40:22] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:40:26] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:40:30] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:40:35] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:40:39] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:40:44] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:40:49] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:40:55] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n   col_num         score\n0       10 -2.267521e+17\n5       15 -2.306571e+17\n7       17 -2.324659e+17\n4       14 -2.352404e+17\n8       18 -2.377501e+17\n3       13 -2.384268e+17\n2       12 -2.389641e+17\n6       16 -2.507131e+17\n9       19 -2.687006e+17\n1       11 -2.820041e+17\nbest col num ::10\n[18:41:00] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\ntarget col ::Y13\n[18:41:05] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:41:25] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:41:32] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:41:37] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:41:41] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:41:46] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:41:50] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:41:55] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:42:00] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:42:06] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:42:11] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n   col_num         score\n8       18 -7.520905e+14\n4       14 -7.822733e+14\n1       11 -7.872308e+14\n2       12 -7.932477e+14\n7       17 -8.085879e+14\n9       19 -8.144192e+14\n3       13 -8.179379e+14\n0       10 -8.278489e+14\n5       15 -8.449964e+14\n6       16 -9.054876e+14\nbest col num ::18\n[18:42:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\ntarget col ::Y14\n[18:42:27] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:42:52] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:42:57] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:43:02] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:43:07] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:43:13] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:43:20] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:43:26] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:43:32] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:43:39] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:43:48] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n   col_num         score\n5       15 -5.954188e+16\n6       16 -7.116683e+16\n4       14 -7.710836e+16\n1       11 -7.766185e+16\n3       13 -8.062348e+16\n9       19 -8.129409e+16\n0       10 -8.282899e+16\n2       12 -8.909105e+16\n8       18 -8.957839e+16\n7       17 -9.549276e+16\nbest col num ::15\n[18:43:57] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\ntarget col ::Y15\n[18:44:05] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:44:29] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:44:34] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:44:38] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:44:42] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:44:48] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:44:53] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:44:58] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:45:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:45:10] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:45:16] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n   col_num         score\n7       17 -6.391404e+15\n6       16 -6.710863e+15\n5       15 -6.968517e+15\n0       10 -7.349044e+15\n4       14 -7.386927e+15\n8       18 -7.722797e+15\n2       12 -8.406416e+15\n1       11 -9.022868e+15\n9       19 -9.198799e+15\n3       13 -1.212232e+16\nbest col num ::17\n[18:45:22] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\ntarget col ::Y16\n[18:45:28] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:45:47] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:45:51] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:45:55] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:46:00] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:46:04] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:46:09] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:46:14] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:46:19] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:46:25] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:46:30] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n   col_num         score\n1       11 -1.258959e+16\n0       10 -1.384486e+16\n8       18 -1.400660e+16\n4       14 -1.445223e+16\n3       13 -1.482268e+16\n2       12 -1.487971e+16\n9       19 -1.488220e+16\n7       17 -1.576820e+16\n5       15 -1.654000e+16\n6       16 -1.685552e+16\nbest col num ::11\n[18:46:37] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\ntarget col ::Y17\n[18:46:41] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:46:59] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:47:03] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:47:07] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:47:11] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:47:15] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:47:19] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:47:24] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:47:29] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:47:34] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[18:47:39] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n   col_num         score\n6       16 -2.131994e+17\n8       18 -2.254898e+17\n5       15 -2.302870e+17\n7       17 -2.421610e+17\n3       13 -2.516375e+17\n4       14 -2.574970e+17\n1       11 -2.588354e+17\n2       12 -2.677481e+17\n0       10 -2.836750e+17\n9       19 -4.233544e+17\nbest col num ::16\n[18:47:45] WARNING: C:/Jenkins/workspace/xgboost-win64_release_0.90/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "target_var_list = ['Y00', 'Y01', 'Y02', 'Y03', 'Y04', 'Y05', 'Y06', 'Y07', 'Y08', 'Y09', 'Y10', 'Y11', 'Y12', 'Y13', 'Y14', 'Y15', 'Y16', 'Y17']\n",
    "features = ['X00', 'X01', 'X02', 'X03', 'X05', 'X06', 'X07', 'X08', 'X09', 'X10', 'X11', 'X12', 'X13', 'X15', 'X17', 'X18', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X37', 'X38', 'X39', 'min']\n",
    "\n",
    "# target_var_list = ['Y00']\n",
    "\n",
    "train_y18 = train[train['Y18'].notnull()]\n",
    "\n",
    "for target_col in target_var_list:\n",
    "    print('target col :: ', target_col)\n",
    "    x_data = trainIsNull[features]\n",
    "    y_data = trainIsNull[target_col]\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, random_state=0)\n",
    "\n",
    "    best_col_num, perm_col_list = check_best_col_num(xgbr, x_val, y_val, trainIsNull, target_col)\n",
    "    print('best col num :: ', best_col_num)\n",
    "\n",
    "    x_train_new, x_val_new, y_train_new, y_val_new = train_test_split(trainIsNull[perm_col_list.iloc[:best_col_num].index], trainIsNull[target_col], random_state=1)\n",
    "\n",
    "    xgbr.fit(x_train_new, y_train_new, eval_set=[(x_val_new, y_val_new)], early_stopping_rounds=100, verbose=False)\n",
    "    train_y18[target_col] = xgbr.predict(train_y18[perm_col_list.iloc[:best_col_num].index])\n",
    "    test[target_col] = xgbr.predict(test[perm_col_list.iloc[:best_col_num].index])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       X00    X01  X02  X03  X04     X05    X06   X07     X08     X09  ...  \\\nid                                                                     ...   \n4320  19.3  987.7  0.9  2.2  0.0  1007.7  988.0  20.8  1007.8  1007.4  ...   \n4321  19.0  987.6  1.9  2.2  0.0  1007.7  988.0  20.6  1007.8  1007.4  ...   \n4322  19.1  987.6  2.0  1.4  0.0  1007.8  988.1  20.5  1007.9  1007.4  ...   \n4323  19.2  987.7  1.8  1.5  0.0  1007.9  988.1  20.5  1007.9  1007.5  ...   \n4324  19.2  987.8  1.4  1.4  0.0  1007.9  988.1  20.5  1007.8  1007.6  ...   \n4325  19.0  987.9  1.5  1.2  0.0  1007.8  988.1  20.4  1008.0  1007.7  ...   \n4326  19.1  987.9  1.2  1.0  0.0  1007.8  988.1  20.3  1008.0  1007.7  ...   \n4327  19.3  987.8  0.7  0.6  0.0  1007.8  988.1  20.4  1008.0  1007.6  ...   \n4328  19.1  987.7  2.7  0.1  0.0  1007.7  988.1  20.4  1008.0  1007.5  ...   \n4329  18.8  987.7  1.2  0.8  0.0  1007.6  988.1  20.4  1008.0  1007.5  ...   \n4330  18.8  987.6  2.0  0.6  0.0  1007.6  988.1  20.4  1007.8  1007.4  ...   \n4331  18.6  987.5  1.2  0.8  0.0  1007.6  988.0  20.4  1007.8  1007.3  ...   \n4332  18.2  987.5  1.4  1.3  0.0  1007.5  988.0  20.4  1007.7  1007.3  ...   \n4333  18.0  987.5  1.8  1.0  0.0  1007.4  987.9  20.4  1007.8  1007.3  ...   \n4334  18.0  987.4  0.5  0.7  0.0  1007.2  987.9  20.4  1007.8  1007.2  ...   \n4335  17.7  987.2  0.2  0.8  0.0  1007.0  987.9  20.4  1007.8  1007.1  ...   \n4336  17.8  987.2  0.8  0.4  0.0  1007.1  988.0  20.3  1007.9  1007.1  ...   \n4337  17.6  987.3  1.1  0.8  0.0  1007.2  988.0  20.2  1007.9  1007.2  ...   \n4338  17.4  987.3  1.1  0.0  0.0  1007.3  987.8  20.1  1007.7  1007.2  ...   \n4339  17.4  987.5  0.5  0.6  0.0  1007.5  987.8  20.0  1007.6  1007.4  ...   \n4340  17.4  987.6  0.6  0.7  0.0  1007.6  987.8  19.9  1007.6  1007.5  ...   \n4341  17.2  987.8  0.4  1.1  0.0  1007.6  987.8  19.9  1007.6  1007.7  ...   \n4342  17.4  987.8  0.2  0.5  0.0  1007.6  987.9  19.9  1007.7  1007.7  ...   \n4343  17.2  987.7  0.5  0.8  0.0  1007.7  987.9  19.9  1007.7  1007.6  ...   \n4344  17.1  987.7  0.5  0.3  0.0  1007.7  987.9  19.9  1007.7  1007.6  ...   \n4345  17.0  987.7  1.1  0.3  0.0  1007.8  988.0  19.9  1007.8  1007.6  ...   \n4346  17.0  987.8  1.2  0.7  0.0  1007.8  988.0  19.9  1007.8  1007.7  ...   \n4347  16.9  987.9  1.0  0.9  0.0  1007.9  988.2  19.8  1008.0  1007.8  ...   \n4348  16.7  987.9  0.9  1.7  0.0  1007.9  988.3  19.6  1008.1  1007.8  ...   \n4349  16.5  988.0  0.3  0.0  0.0  1007.8  988.3  19.5  1008.1  1007.9  ...   \n...    ...    ...  ...  ...  ...     ...    ...   ...     ...     ...  ...   \n4722  25.7  985.3  0.3  0.8  0.0  1004.4  985.8  25.0  1005.4  1004.7  ...   \n4723  25.4  985.4  2.2  0.0  0.0  1004.4  985.8  24.5  1005.5  1004.8  ...   \n4724  25.2  985.5  0.7  0.7  0.0  1004.4  986.2  24.1  1005.9  1005.0  ...   \n4725  25.2  985.7  1.7  1.5  0.0  1004.6  986.1  23.8  1005.8  1005.2  ...   \n4726  25.0  985.6  1.5  1.7  0.0  1004.6  986.0  23.5  1005.7  1005.2  ...   \n4727  24.9  985.8  1.6  1.0  0.0  1004.8  986.4  23.3  1006.1  1005.4  ...   \n4728  24.7  985.9  2.2  1.4  0.0  1004.9  986.4  23.2  1006.1  1005.5  ...   \n4729  24.5  986.0  2.2  1.0  0.0  1005.0  986.6  23.1  1006.3  1005.7  ...   \n4730  24.2  986.1  1.4  1.0  0.0  1005.1  986.7  23.0  1006.5  1005.8  ...   \n4731  24.0  986.1  1.0  1.3  0.0  1005.3  986.7  23.0  1006.5  1005.7  ...   \n4732  24.1  986.5  1.2  1.5  0.0  1005.4  986.7  23.0  1006.5  1006.0  ...   \n4733  23.4  986.9  1.5  0.4  0.0  1005.6  986.7  23.0  1006.5  1006.5  ...   \n4734  23.2  987.0  0.8  0.8  0.0  1005.7  987.2  23.0  1007.1  1006.7  ...   \n4735  22.0  987.2  1.3  0.6  0.0  1005.8  987.3  22.8  1007.2  1006.8  ...   \n4736  21.7  987.3  2.5  2.2  0.0  1006.0  987.5  22.6  1007.4  1006.9  ...   \n4737  21.6  987.5  1.8  2.6  0.0  1006.1  987.7  22.5  1007.6  1007.2  ...   \n4738  21.5  987.7  2.0  2.4  0.0  1006.3  987.7  22.5  1007.6  1007.4  ...   \n4739  21.3  987.7  2.6  2.4  0.0  1006.4  987.7  22.4  1007.6  1007.4  ...   \n4740  21.1  987.7  1.4  2.7  0.0  1006.6  987.9  22.3  1007.8  1007.4  ...   \n4741  21.0  987.7  1.6  2.0  0.0  1006.6  987.9  22.2  1007.8  1007.4  ...   \n4742  20.9  987.7  0.2  2.0  0.0  1006.6  987.8  22.2  1007.7  1007.4  ...   \n4743  20.8  987.6  2.6  2.0  0.0  1006.8  987.6  22.0  1007.3  1007.3  ...   \n4744  20.5  987.7  1.1  2.0  0.0  1006.8  987.8  21.9  1007.7  1007.4  ...   \n4745  20.2  987.8  1.1  1.6  0.0  1006.9  987.8  21.7  1007.7  1007.6  ...   \n4746  20.1  987.9  0.3  1.1  0.0  1006.9  987.7  21.7  1007.4  1007.7  ...   \n4747  19.9  987.6  0.9  0.8  0.0  1006.9  987.7  21.7  1007.5  1007.4  ...   \n4748  19.9  987.6  0.5  0.7  0.0  1006.8  987.7  21.6  1007.5  1007.4  ...   \n4749  19.7  987.7  0.9  0.6  0.0  1006.9  987.6  21.4  1007.4  1007.5  ...   \n4750  19.4  987.7  0.9  0.8  0.0  1006.9  987.8  21.3  1007.6  1007.5  ...   \n4751  19.1  987.6  1.0  0.3  0.0  1006.8  987.8  21.2  1007.5  1007.4  ...   \n\n            Y10        Y11        Y12        Y13        Y14        Y15  \\\nid                                                                       \n4320  19.235428  18.011063  18.785843  19.100283  18.818462  18.864697   \n4321  18.998430  18.059540  18.944035  18.961758  18.895243  18.608625   \n4322  18.921997  17.734970  18.990133  18.812595  18.593117  18.661751   \n4323  19.075909  17.969610  19.186008  18.804304  18.582626  18.487194   \n4324  19.167698  18.118151  19.230677  18.976982  18.605314  18.566994   \n4325  18.873762  17.872196  19.140951  19.080946  18.605314  18.505583   \n4326  19.195274  18.109852  19.014959  19.119270  18.746696  18.943382   \n4327  19.054167  18.027884  19.089655  19.127529  18.733000  19.061148   \n4328  19.118275  18.074217  19.062916  19.153437  18.749903  19.023693   \n4329  18.680033  17.927664  19.059134  19.043219  18.519201  18.365210   \n4330  18.815485  17.883657  18.820208  18.966803  18.530218  18.266184   \n4331  18.509405  17.687468  18.962149  18.667690  18.503489  18.233255   \n4332  19.001516  17.990286  19.079470  18.928654  18.526449  18.208740   \n4333  18.770962  17.910614  19.304487  18.955982  18.575050  18.375271   \n4334  18.856089  17.883553  19.264688  18.954683  18.557178  18.326591   \n4335  18.478251  17.587578  19.615660  19.115870  18.641237  18.230484   \n4336  18.650204  17.739607  19.337933  19.098049  18.783323  18.733255   \n4337  18.437201  17.691223  19.124868  19.110739  18.735760  18.793472   \n4338  18.013727  17.573458  18.869530  18.911507  18.491341  18.268610   \n4339  17.512661  16.993866  18.521858  18.270466  18.111397  18.097746   \n4340  17.646954  17.003096  18.871365  18.295301  18.262573  18.203856   \n4341  17.625551  16.996754  18.893848  18.337076  18.262703  18.001310   \n4342  17.541218  16.974644  18.654667  18.374548  18.285639  17.982534   \n4343  17.602299  16.897888  18.794477  18.499308  18.506542  18.142704   \n4344  18.015289  17.168859  18.640591  18.527489  18.542274  18.249506   \n4345  18.262762  17.172607  18.830072  18.437988  18.512102  18.355568   \n4346  17.716415  16.924032  18.835556  18.531551  18.530519  18.479336   \n4347  17.472414  16.859764  18.289339  18.627485  18.408699  18.292017   \n4348  17.418329  16.764048  18.155161  18.596252  18.361099  18.347338   \n4349  17.304310  16.858555  18.130371  18.645479  18.345539  18.478781   \n...         ...        ...        ...        ...        ...        ...   \n4722  23.909901  25.557949  24.146143  22.321728  24.209221  21.080130   \n4723  23.283443  25.065630  24.143764  21.612982  22.767118  21.045340   \n4724  23.046352  24.947683  23.628710  21.165129  21.376652  20.805553   \n4725  22.335890  24.450394  22.927778  20.839725  21.048243  19.796761   \n4726  22.012934  24.154810  21.690397  20.715872  20.715956  20.010736   \n4727  22.126944  24.522858  21.916485  20.194939  20.118700  20.031036   \n4728  22.713156  24.745487  21.458591  19.819065  20.448814  20.236511   \n4729  22.491228  22.805273  21.096832  19.739388  20.469873  19.894411   \n4730  21.978525  23.963663  21.144520  19.433830  19.988476  19.601048   \n4731  22.290283  24.331848  21.289675  20.286484  20.323788  19.704828   \n4732  22.826239  24.252520  21.241669  20.369736  21.026825  19.587795   \n4733  22.365171  23.676107  21.582088  20.417210  21.047569  20.110048   \n4734  22.230181  23.382536  21.134792  20.372047  20.887573  20.305517   \n4735  21.489285  22.266048  21.000013  20.479671  20.409552  20.125942   \n4736  21.471090  22.217978  20.597713  20.390404  19.640066  20.054193   \n4737  21.110674  21.274258  20.830555  20.303028  19.857246  19.893646   \n4738  21.197664  21.403278  20.986320  20.374630  19.837166  20.076649   \n4739  21.104771  20.797606  20.561764  20.272457  20.216244  19.767256   \n4740  20.888376  20.690214  20.658377  20.277390  19.997856  19.573341   \n4741  20.711042  20.539783  20.613564  20.181747  19.734524  19.627962   \n4742  20.316914  20.675463  20.586403  20.296913  19.537392  19.674313   \n4743  20.263968  20.372398  20.113317  20.311310  19.675192  19.656439   \n4744  20.573229  20.376558  20.527258  19.958427  19.486250  19.645269   \n4745  20.206909  19.749041  20.498755  19.816803  18.971029  19.155348   \n4746  20.200026  19.740654  20.108601  19.909378  19.142656  19.213314   \n4747  20.014666  19.676689  20.255836  19.621681  19.100281  19.040098   \n4748  19.562740  19.893084  20.044662  19.696766  19.123640  19.247835   \n4749  19.389511  19.311953  20.230379  19.535376  19.093515  19.028814   \n4750  19.401882  19.472687  19.696304  19.418423  18.642723  18.986610   \n4751  19.455137  19.701723  19.360321  19.589153  18.591835  18.894979   \n\n            Y16        Y17   Y18  min  \nid                                     \n4320  18.864325  18.235130  20.5    0  \n4321  19.028246  18.421999  20.5    1  \n4322  18.332323  18.745970  20.5    2  \n4323  18.536533  18.570435  20.5    3  \n4324  18.252573  18.678518  20.5    4  \n4325  18.437763  18.644232  20.5    5  \n4326  18.578840  18.611666  20.5    6  \n4327  18.586298  18.684757  20.5    7  \n4328  18.865437  18.560026  20.5    8  \n4329  18.420572  18.614449  20.5    9  \n4330  18.388348  18.451403  20.5   10  \n4331  18.327932  18.255281  20.5   11  \n4332  18.054710  18.567623  20.5   12  \n4333  18.417952  18.797075  20.5   13  \n4334  18.484016  18.839926  20.5   14  \n4335  18.681673  18.908157  20.5   15  \n4336  19.036604  18.664572  20.0   16  \n4337  18.853415  18.675678  20.0   17  \n4338  18.420452  18.676397  20.0   18  \n4339  18.044865  18.194275  19.5   19  \n4340  18.168707  18.219931  19.0   20  \n4341  18.147858  18.152237  19.0   21  \n4342  18.142174  17.915897  19.0   22  \n4343  18.211569  18.018688  19.0   23  \n4344  18.039572  18.030499  19.0   24  \n4345  18.183807  18.247026  19.0   25  \n4346  18.183807  18.264893  19.0   26  \n4347  17.845617  18.041504  19.0   27  \n4348  17.817616  18.041504  19.0   28  \n4349  17.745161  18.184973  19.0   29  \n...         ...        ...   ...  ...  \n4722  25.009521  23.088800  26.0  114  \n4723  24.652887  22.049940  25.5  115  \n4724  24.737173  21.202312  25.5  116  \n4725  23.973196  20.807983  25.0  117  \n4726  22.072844  19.450621  24.5  118  \n4727  22.050852  18.634809  24.5  119  \n4728  21.825460  18.627455  24.0  120  \n4729  21.596004  18.089945  24.0  121  \n4730  21.394819  17.889400  23.5  122  \n4731  21.076162  18.721809  23.5  123  \n4732  21.278210  19.006927  23.0  124  \n4733  21.254208  18.947470  23.0  125  \n4734  20.893341  18.788412  23.0  126  \n4735  20.322477  18.719667  23.0  127  \n4736  20.153816  18.622232  23.0  128  \n4737  20.277950  18.283499  23.0  129  \n4738  20.027288  18.613560  23.0  130  \n4739  20.061575  19.214008  23.0  131  \n4740  19.843885  19.222441  23.0  132  \n4741  19.680889  19.236675  22.5  133  \n4742  19.456739  19.162935  22.5  134  \n4743  19.416220  18.821072  22.0  135  \n4744  19.902588  18.955053  22.0  136  \n4745  19.565237  18.668520  22.0  137  \n4746  19.442717  18.589371  22.0  138  \n4747  19.484961  18.347324  21.5  139  \n4748  19.475515  18.564573  21.5  140  \n4749  19.406361  18.389347  21.5  141  \n4750  19.291885  18.357679  21.5  142  \n4751  19.210253  18.406981  21.0  143  \n\n[432 rows x 60 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X00</th>\n      <th>X01</th>\n      <th>X02</th>\n      <th>X03</th>\n      <th>X04</th>\n      <th>X05</th>\n      <th>X06</th>\n      <th>X07</th>\n      <th>X08</th>\n      <th>X09</th>\n      <th>...</th>\n      <th>Y10</th>\n      <th>Y11</th>\n      <th>Y12</th>\n      <th>Y13</th>\n      <th>Y14</th>\n      <th>Y15</th>\n      <th>Y16</th>\n      <th>Y17</th>\n      <th>Y18</th>\n      <th>min</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4320</th>\n      <td>19.3</td>\n      <td>987.7</td>\n      <td>0.9</td>\n      <td>2.2</td>\n      <td>0.0</td>\n      <td>1007.7</td>\n      <td>988.0</td>\n      <td>20.8</td>\n      <td>1007.8</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>19.235428</td>\n      <td>18.011063</td>\n      <td>18.785843</td>\n      <td>19.100283</td>\n      <td>18.818462</td>\n      <td>18.864697</td>\n      <td>18.864325</td>\n      <td>18.235130</td>\n      <td>20.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4321</th>\n      <td>19.0</td>\n      <td>987.6</td>\n      <td>1.9</td>\n      <td>2.2</td>\n      <td>0.0</td>\n      <td>1007.7</td>\n      <td>988.0</td>\n      <td>20.6</td>\n      <td>1007.8</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>18.998430</td>\n      <td>18.059540</td>\n      <td>18.944035</td>\n      <td>18.961758</td>\n      <td>18.895243</td>\n      <td>18.608625</td>\n      <td>19.028246</td>\n      <td>18.421999</td>\n      <td>20.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4322</th>\n      <td>19.1</td>\n      <td>987.6</td>\n      <td>2.0</td>\n      <td>1.4</td>\n      <td>0.0</td>\n      <td>1007.8</td>\n      <td>988.1</td>\n      <td>20.5</td>\n      <td>1007.9</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>18.921997</td>\n      <td>17.734970</td>\n      <td>18.990133</td>\n      <td>18.812595</td>\n      <td>18.593117</td>\n      <td>18.661751</td>\n      <td>18.332323</td>\n      <td>18.745970</td>\n      <td>20.5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4323</th>\n      <td>19.2</td>\n      <td>987.7</td>\n      <td>1.8</td>\n      <td>1.5</td>\n      <td>0.0</td>\n      <td>1007.9</td>\n      <td>988.1</td>\n      <td>20.5</td>\n      <td>1007.9</td>\n      <td>1007.5</td>\n      <td>...</td>\n      <td>19.075909</td>\n      <td>17.969610</td>\n      <td>19.186008</td>\n      <td>18.804304</td>\n      <td>18.582626</td>\n      <td>18.487194</td>\n      <td>18.536533</td>\n      <td>18.570435</td>\n      <td>20.5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4324</th>\n      <td>19.2</td>\n      <td>987.8</td>\n      <td>1.4</td>\n      <td>1.4</td>\n      <td>0.0</td>\n      <td>1007.9</td>\n      <td>988.1</td>\n      <td>20.5</td>\n      <td>1007.8</td>\n      <td>1007.6</td>\n      <td>...</td>\n      <td>19.167698</td>\n      <td>18.118151</td>\n      <td>19.230677</td>\n      <td>18.976982</td>\n      <td>18.605314</td>\n      <td>18.566994</td>\n      <td>18.252573</td>\n      <td>18.678518</td>\n      <td>20.5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4325</th>\n      <td>19.0</td>\n      <td>987.9</td>\n      <td>1.5</td>\n      <td>1.2</td>\n      <td>0.0</td>\n      <td>1007.8</td>\n      <td>988.1</td>\n      <td>20.4</td>\n      <td>1008.0</td>\n      <td>1007.7</td>\n      <td>...</td>\n      <td>18.873762</td>\n      <td>17.872196</td>\n      <td>19.140951</td>\n      <td>19.080946</td>\n      <td>18.605314</td>\n      <td>18.505583</td>\n      <td>18.437763</td>\n      <td>18.644232</td>\n      <td>20.5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4326</th>\n      <td>19.1</td>\n      <td>987.9</td>\n      <td>1.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1007.8</td>\n      <td>988.1</td>\n      <td>20.3</td>\n      <td>1008.0</td>\n      <td>1007.7</td>\n      <td>...</td>\n      <td>19.195274</td>\n      <td>18.109852</td>\n      <td>19.014959</td>\n      <td>19.119270</td>\n      <td>18.746696</td>\n      <td>18.943382</td>\n      <td>18.578840</td>\n      <td>18.611666</td>\n      <td>20.5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4327</th>\n      <td>19.3</td>\n      <td>987.8</td>\n      <td>0.7</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>1007.8</td>\n      <td>988.1</td>\n      <td>20.4</td>\n      <td>1008.0</td>\n      <td>1007.6</td>\n      <td>...</td>\n      <td>19.054167</td>\n      <td>18.027884</td>\n      <td>19.089655</td>\n      <td>19.127529</td>\n      <td>18.733000</td>\n      <td>19.061148</td>\n      <td>18.586298</td>\n      <td>18.684757</td>\n      <td>20.5</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4328</th>\n      <td>19.1</td>\n      <td>987.7</td>\n      <td>2.7</td>\n      <td>0.1</td>\n      <td>0.0</td>\n      <td>1007.7</td>\n      <td>988.1</td>\n      <td>20.4</td>\n      <td>1008.0</td>\n      <td>1007.5</td>\n      <td>...</td>\n      <td>19.118275</td>\n      <td>18.074217</td>\n      <td>19.062916</td>\n      <td>19.153437</td>\n      <td>18.749903</td>\n      <td>19.023693</td>\n      <td>18.865437</td>\n      <td>18.560026</td>\n      <td>20.5</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4329</th>\n      <td>18.8</td>\n      <td>987.7</td>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>1007.6</td>\n      <td>988.1</td>\n      <td>20.4</td>\n      <td>1008.0</td>\n      <td>1007.5</td>\n      <td>...</td>\n      <td>18.680033</td>\n      <td>17.927664</td>\n      <td>19.059134</td>\n      <td>19.043219</td>\n      <td>18.519201</td>\n      <td>18.365210</td>\n      <td>18.420572</td>\n      <td>18.614449</td>\n      <td>20.5</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4330</th>\n      <td>18.8</td>\n      <td>987.6</td>\n      <td>2.0</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>1007.6</td>\n      <td>988.1</td>\n      <td>20.4</td>\n      <td>1007.8</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>18.815485</td>\n      <td>17.883657</td>\n      <td>18.820208</td>\n      <td>18.966803</td>\n      <td>18.530218</td>\n      <td>18.266184</td>\n      <td>18.388348</td>\n      <td>18.451403</td>\n      <td>20.5</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4331</th>\n      <td>18.6</td>\n      <td>987.5</td>\n      <td>1.2</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>1007.6</td>\n      <td>988.0</td>\n      <td>20.4</td>\n      <td>1007.8</td>\n      <td>1007.3</td>\n      <td>...</td>\n      <td>18.509405</td>\n      <td>17.687468</td>\n      <td>18.962149</td>\n      <td>18.667690</td>\n      <td>18.503489</td>\n      <td>18.233255</td>\n      <td>18.327932</td>\n      <td>18.255281</td>\n      <td>20.5</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4332</th>\n      <td>18.2</td>\n      <td>987.5</td>\n      <td>1.4</td>\n      <td>1.3</td>\n      <td>0.0</td>\n      <td>1007.5</td>\n      <td>988.0</td>\n      <td>20.4</td>\n      <td>1007.7</td>\n      <td>1007.3</td>\n      <td>...</td>\n      <td>19.001516</td>\n      <td>17.990286</td>\n      <td>19.079470</td>\n      <td>18.928654</td>\n      <td>18.526449</td>\n      <td>18.208740</td>\n      <td>18.054710</td>\n      <td>18.567623</td>\n      <td>20.5</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>4333</th>\n      <td>18.0</td>\n      <td>987.5</td>\n      <td>1.8</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1007.4</td>\n      <td>987.9</td>\n      <td>20.4</td>\n      <td>1007.8</td>\n      <td>1007.3</td>\n      <td>...</td>\n      <td>18.770962</td>\n      <td>17.910614</td>\n      <td>19.304487</td>\n      <td>18.955982</td>\n      <td>18.575050</td>\n      <td>18.375271</td>\n      <td>18.417952</td>\n      <td>18.797075</td>\n      <td>20.5</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>4334</th>\n      <td>18.0</td>\n      <td>987.4</td>\n      <td>0.5</td>\n      <td>0.7</td>\n      <td>0.0</td>\n      <td>1007.2</td>\n      <td>987.9</td>\n      <td>20.4</td>\n      <td>1007.8</td>\n      <td>1007.2</td>\n      <td>...</td>\n      <td>18.856089</td>\n      <td>17.883553</td>\n      <td>19.264688</td>\n      <td>18.954683</td>\n      <td>18.557178</td>\n      <td>18.326591</td>\n      <td>18.484016</td>\n      <td>18.839926</td>\n      <td>20.5</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>4335</th>\n      <td>17.7</td>\n      <td>987.2</td>\n      <td>0.2</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>1007.0</td>\n      <td>987.9</td>\n      <td>20.4</td>\n      <td>1007.8</td>\n      <td>1007.1</td>\n      <td>...</td>\n      <td>18.478251</td>\n      <td>17.587578</td>\n      <td>19.615660</td>\n      <td>19.115870</td>\n      <td>18.641237</td>\n      <td>18.230484</td>\n      <td>18.681673</td>\n      <td>18.908157</td>\n      <td>20.5</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>4336</th>\n      <td>17.8</td>\n      <td>987.2</td>\n      <td>0.8</td>\n      <td>0.4</td>\n      <td>0.0</td>\n      <td>1007.1</td>\n      <td>988.0</td>\n      <td>20.3</td>\n      <td>1007.9</td>\n      <td>1007.1</td>\n      <td>...</td>\n      <td>18.650204</td>\n      <td>17.739607</td>\n      <td>19.337933</td>\n      <td>19.098049</td>\n      <td>18.783323</td>\n      <td>18.733255</td>\n      <td>19.036604</td>\n      <td>18.664572</td>\n      <td>20.0</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>4337</th>\n      <td>17.6</td>\n      <td>987.3</td>\n      <td>1.1</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>1007.2</td>\n      <td>988.0</td>\n      <td>20.2</td>\n      <td>1007.9</td>\n      <td>1007.2</td>\n      <td>...</td>\n      <td>18.437201</td>\n      <td>17.691223</td>\n      <td>19.124868</td>\n      <td>19.110739</td>\n      <td>18.735760</td>\n      <td>18.793472</td>\n      <td>18.853415</td>\n      <td>18.675678</td>\n      <td>20.0</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>4338</th>\n      <td>17.4</td>\n      <td>987.3</td>\n      <td>1.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1007.3</td>\n      <td>987.8</td>\n      <td>20.1</td>\n      <td>1007.7</td>\n      <td>1007.2</td>\n      <td>...</td>\n      <td>18.013727</td>\n      <td>17.573458</td>\n      <td>18.869530</td>\n      <td>18.911507</td>\n      <td>18.491341</td>\n      <td>18.268610</td>\n      <td>18.420452</td>\n      <td>18.676397</td>\n      <td>20.0</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>4339</th>\n      <td>17.4</td>\n      <td>987.5</td>\n      <td>0.5</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>1007.5</td>\n      <td>987.8</td>\n      <td>20.0</td>\n      <td>1007.6</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>17.512661</td>\n      <td>16.993866</td>\n      <td>18.521858</td>\n      <td>18.270466</td>\n      <td>18.111397</td>\n      <td>18.097746</td>\n      <td>18.044865</td>\n      <td>18.194275</td>\n      <td>19.5</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4340</th>\n      <td>17.4</td>\n      <td>987.6</td>\n      <td>0.6</td>\n      <td>0.7</td>\n      <td>0.0</td>\n      <td>1007.6</td>\n      <td>987.8</td>\n      <td>19.9</td>\n      <td>1007.6</td>\n      <td>1007.5</td>\n      <td>...</td>\n      <td>17.646954</td>\n      <td>17.003096</td>\n      <td>18.871365</td>\n      <td>18.295301</td>\n      <td>18.262573</td>\n      <td>18.203856</td>\n      <td>18.168707</td>\n      <td>18.219931</td>\n      <td>19.0</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4341</th>\n      <td>17.2</td>\n      <td>987.8</td>\n      <td>0.4</td>\n      <td>1.1</td>\n      <td>0.0</td>\n      <td>1007.6</td>\n      <td>987.8</td>\n      <td>19.9</td>\n      <td>1007.6</td>\n      <td>1007.7</td>\n      <td>...</td>\n      <td>17.625551</td>\n      <td>16.996754</td>\n      <td>18.893848</td>\n      <td>18.337076</td>\n      <td>18.262703</td>\n      <td>18.001310</td>\n      <td>18.147858</td>\n      <td>18.152237</td>\n      <td>19.0</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>4342</th>\n      <td>17.4</td>\n      <td>987.8</td>\n      <td>0.2</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>1007.6</td>\n      <td>987.9</td>\n      <td>19.9</td>\n      <td>1007.7</td>\n      <td>1007.7</td>\n      <td>...</td>\n      <td>17.541218</td>\n      <td>16.974644</td>\n      <td>18.654667</td>\n      <td>18.374548</td>\n      <td>18.285639</td>\n      <td>17.982534</td>\n      <td>18.142174</td>\n      <td>17.915897</td>\n      <td>19.0</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>4343</th>\n      <td>17.2</td>\n      <td>987.7</td>\n      <td>0.5</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>1007.7</td>\n      <td>987.9</td>\n      <td>19.9</td>\n      <td>1007.7</td>\n      <td>1007.6</td>\n      <td>...</td>\n      <td>17.602299</td>\n      <td>16.897888</td>\n      <td>18.794477</td>\n      <td>18.499308</td>\n      <td>18.506542</td>\n      <td>18.142704</td>\n      <td>18.211569</td>\n      <td>18.018688</td>\n      <td>19.0</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4344</th>\n      <td>17.1</td>\n      <td>987.7</td>\n      <td>0.5</td>\n      <td>0.3</td>\n      <td>0.0</td>\n      <td>1007.7</td>\n      <td>987.9</td>\n      <td>19.9</td>\n      <td>1007.7</td>\n      <td>1007.6</td>\n      <td>...</td>\n      <td>18.015289</td>\n      <td>17.168859</td>\n      <td>18.640591</td>\n      <td>18.527489</td>\n      <td>18.542274</td>\n      <td>18.249506</td>\n      <td>18.039572</td>\n      <td>18.030499</td>\n      <td>19.0</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>4345</th>\n      <td>17.0</td>\n      <td>987.7</td>\n      <td>1.1</td>\n      <td>0.3</td>\n      <td>0.0</td>\n      <td>1007.8</td>\n      <td>988.0</td>\n      <td>19.9</td>\n      <td>1007.8</td>\n      <td>1007.6</td>\n      <td>...</td>\n      <td>18.262762</td>\n      <td>17.172607</td>\n      <td>18.830072</td>\n      <td>18.437988</td>\n      <td>18.512102</td>\n      <td>18.355568</td>\n      <td>18.183807</td>\n      <td>18.247026</td>\n      <td>19.0</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>4346</th>\n      <td>17.0</td>\n      <td>987.8</td>\n      <td>1.2</td>\n      <td>0.7</td>\n      <td>0.0</td>\n      <td>1007.8</td>\n      <td>988.0</td>\n      <td>19.9</td>\n      <td>1007.8</td>\n      <td>1007.7</td>\n      <td>...</td>\n      <td>17.716415</td>\n      <td>16.924032</td>\n      <td>18.835556</td>\n      <td>18.531551</td>\n      <td>18.530519</td>\n      <td>18.479336</td>\n      <td>18.183807</td>\n      <td>18.264893</td>\n      <td>19.0</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>4347</th>\n      <td>16.9</td>\n      <td>987.9</td>\n      <td>1.0</td>\n      <td>0.9</td>\n      <td>0.0</td>\n      <td>1007.9</td>\n      <td>988.2</td>\n      <td>19.8</td>\n      <td>1008.0</td>\n      <td>1007.8</td>\n      <td>...</td>\n      <td>17.472414</td>\n      <td>16.859764</td>\n      <td>18.289339</td>\n      <td>18.627485</td>\n      <td>18.408699</td>\n      <td>18.292017</td>\n      <td>17.845617</td>\n      <td>18.041504</td>\n      <td>19.0</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>4348</th>\n      <td>16.7</td>\n      <td>987.9</td>\n      <td>0.9</td>\n      <td>1.7</td>\n      <td>0.0</td>\n      <td>1007.9</td>\n      <td>988.3</td>\n      <td>19.6</td>\n      <td>1008.1</td>\n      <td>1007.8</td>\n      <td>...</td>\n      <td>17.418329</td>\n      <td>16.764048</td>\n      <td>18.155161</td>\n      <td>18.596252</td>\n      <td>18.361099</td>\n      <td>18.347338</td>\n      <td>17.817616</td>\n      <td>18.041504</td>\n      <td>19.0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>4349</th>\n      <td>16.5</td>\n      <td>988.0</td>\n      <td>0.3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1007.8</td>\n      <td>988.3</td>\n      <td>19.5</td>\n      <td>1008.1</td>\n      <td>1007.9</td>\n      <td>...</td>\n      <td>17.304310</td>\n      <td>16.858555</td>\n      <td>18.130371</td>\n      <td>18.645479</td>\n      <td>18.345539</td>\n      <td>18.478781</td>\n      <td>17.745161</td>\n      <td>18.184973</td>\n      <td>19.0</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4722</th>\n      <td>25.7</td>\n      <td>985.3</td>\n      <td>0.3</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>1004.4</td>\n      <td>985.8</td>\n      <td>25.0</td>\n      <td>1005.4</td>\n      <td>1004.7</td>\n      <td>...</td>\n      <td>23.909901</td>\n      <td>25.557949</td>\n      <td>24.146143</td>\n      <td>22.321728</td>\n      <td>24.209221</td>\n      <td>21.080130</td>\n      <td>25.009521</td>\n      <td>23.088800</td>\n      <td>26.0</td>\n      <td>114</td>\n    </tr>\n    <tr>\n      <th>4723</th>\n      <td>25.4</td>\n      <td>985.4</td>\n      <td>2.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1004.4</td>\n      <td>985.8</td>\n      <td>24.5</td>\n      <td>1005.5</td>\n      <td>1004.8</td>\n      <td>...</td>\n      <td>23.283443</td>\n      <td>25.065630</td>\n      <td>24.143764</td>\n      <td>21.612982</td>\n      <td>22.767118</td>\n      <td>21.045340</td>\n      <td>24.652887</td>\n      <td>22.049940</td>\n      <td>25.5</td>\n      <td>115</td>\n    </tr>\n    <tr>\n      <th>4724</th>\n      <td>25.2</td>\n      <td>985.5</td>\n      <td>0.7</td>\n      <td>0.7</td>\n      <td>0.0</td>\n      <td>1004.4</td>\n      <td>986.2</td>\n      <td>24.1</td>\n      <td>1005.9</td>\n      <td>1005.0</td>\n      <td>...</td>\n      <td>23.046352</td>\n      <td>24.947683</td>\n      <td>23.628710</td>\n      <td>21.165129</td>\n      <td>21.376652</td>\n      <td>20.805553</td>\n      <td>24.737173</td>\n      <td>21.202312</td>\n      <td>25.5</td>\n      <td>116</td>\n    </tr>\n    <tr>\n      <th>4725</th>\n      <td>25.2</td>\n      <td>985.7</td>\n      <td>1.7</td>\n      <td>1.5</td>\n      <td>0.0</td>\n      <td>1004.6</td>\n      <td>986.1</td>\n      <td>23.8</td>\n      <td>1005.8</td>\n      <td>1005.2</td>\n      <td>...</td>\n      <td>22.335890</td>\n      <td>24.450394</td>\n      <td>22.927778</td>\n      <td>20.839725</td>\n      <td>21.048243</td>\n      <td>19.796761</td>\n      <td>23.973196</td>\n      <td>20.807983</td>\n      <td>25.0</td>\n      <td>117</td>\n    </tr>\n    <tr>\n      <th>4726</th>\n      <td>25.0</td>\n      <td>985.6</td>\n      <td>1.5</td>\n      <td>1.7</td>\n      <td>0.0</td>\n      <td>1004.6</td>\n      <td>986.0</td>\n      <td>23.5</td>\n      <td>1005.7</td>\n      <td>1005.2</td>\n      <td>...</td>\n      <td>22.012934</td>\n      <td>24.154810</td>\n      <td>21.690397</td>\n      <td>20.715872</td>\n      <td>20.715956</td>\n      <td>20.010736</td>\n      <td>22.072844</td>\n      <td>19.450621</td>\n      <td>24.5</td>\n      <td>118</td>\n    </tr>\n    <tr>\n      <th>4727</th>\n      <td>24.9</td>\n      <td>985.8</td>\n      <td>1.6</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1004.8</td>\n      <td>986.4</td>\n      <td>23.3</td>\n      <td>1006.1</td>\n      <td>1005.4</td>\n      <td>...</td>\n      <td>22.126944</td>\n      <td>24.522858</td>\n      <td>21.916485</td>\n      <td>20.194939</td>\n      <td>20.118700</td>\n      <td>20.031036</td>\n      <td>22.050852</td>\n      <td>18.634809</td>\n      <td>24.5</td>\n      <td>119</td>\n    </tr>\n    <tr>\n      <th>4728</th>\n      <td>24.7</td>\n      <td>985.9</td>\n      <td>2.2</td>\n      <td>1.4</td>\n      <td>0.0</td>\n      <td>1004.9</td>\n      <td>986.4</td>\n      <td>23.2</td>\n      <td>1006.1</td>\n      <td>1005.5</td>\n      <td>...</td>\n      <td>22.713156</td>\n      <td>24.745487</td>\n      <td>21.458591</td>\n      <td>19.819065</td>\n      <td>20.448814</td>\n      <td>20.236511</td>\n      <td>21.825460</td>\n      <td>18.627455</td>\n      <td>24.0</td>\n      <td>120</td>\n    </tr>\n    <tr>\n      <th>4729</th>\n      <td>24.5</td>\n      <td>986.0</td>\n      <td>2.2</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1005.0</td>\n      <td>986.6</td>\n      <td>23.1</td>\n      <td>1006.3</td>\n      <td>1005.7</td>\n      <td>...</td>\n      <td>22.491228</td>\n      <td>22.805273</td>\n      <td>21.096832</td>\n      <td>19.739388</td>\n      <td>20.469873</td>\n      <td>19.894411</td>\n      <td>21.596004</td>\n      <td>18.089945</td>\n      <td>24.0</td>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>4730</th>\n      <td>24.2</td>\n      <td>986.1</td>\n      <td>1.4</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1005.1</td>\n      <td>986.7</td>\n      <td>23.0</td>\n      <td>1006.5</td>\n      <td>1005.8</td>\n      <td>...</td>\n      <td>21.978525</td>\n      <td>23.963663</td>\n      <td>21.144520</td>\n      <td>19.433830</td>\n      <td>19.988476</td>\n      <td>19.601048</td>\n      <td>21.394819</td>\n      <td>17.889400</td>\n      <td>23.5</td>\n      <td>122</td>\n    </tr>\n    <tr>\n      <th>4731</th>\n      <td>24.0</td>\n      <td>986.1</td>\n      <td>1.0</td>\n      <td>1.3</td>\n      <td>0.0</td>\n      <td>1005.3</td>\n      <td>986.7</td>\n      <td>23.0</td>\n      <td>1006.5</td>\n      <td>1005.7</td>\n      <td>...</td>\n      <td>22.290283</td>\n      <td>24.331848</td>\n      <td>21.289675</td>\n      <td>20.286484</td>\n      <td>20.323788</td>\n      <td>19.704828</td>\n      <td>21.076162</td>\n      <td>18.721809</td>\n      <td>23.5</td>\n      <td>123</td>\n    </tr>\n    <tr>\n      <th>4732</th>\n      <td>24.1</td>\n      <td>986.5</td>\n      <td>1.2</td>\n      <td>1.5</td>\n      <td>0.0</td>\n      <td>1005.4</td>\n      <td>986.7</td>\n      <td>23.0</td>\n      <td>1006.5</td>\n      <td>1006.0</td>\n      <td>...</td>\n      <td>22.826239</td>\n      <td>24.252520</td>\n      <td>21.241669</td>\n      <td>20.369736</td>\n      <td>21.026825</td>\n      <td>19.587795</td>\n      <td>21.278210</td>\n      <td>19.006927</td>\n      <td>23.0</td>\n      <td>124</td>\n    </tr>\n    <tr>\n      <th>4733</th>\n      <td>23.4</td>\n      <td>986.9</td>\n      <td>1.5</td>\n      <td>0.4</td>\n      <td>0.0</td>\n      <td>1005.6</td>\n      <td>986.7</td>\n      <td>23.0</td>\n      <td>1006.5</td>\n      <td>1006.5</td>\n      <td>...</td>\n      <td>22.365171</td>\n      <td>23.676107</td>\n      <td>21.582088</td>\n      <td>20.417210</td>\n      <td>21.047569</td>\n      <td>20.110048</td>\n      <td>21.254208</td>\n      <td>18.947470</td>\n      <td>23.0</td>\n      <td>125</td>\n    </tr>\n    <tr>\n      <th>4734</th>\n      <td>23.2</td>\n      <td>987.0</td>\n      <td>0.8</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>1005.7</td>\n      <td>987.2</td>\n      <td>23.0</td>\n      <td>1007.1</td>\n      <td>1006.7</td>\n      <td>...</td>\n      <td>22.230181</td>\n      <td>23.382536</td>\n      <td>21.134792</td>\n      <td>20.372047</td>\n      <td>20.887573</td>\n      <td>20.305517</td>\n      <td>20.893341</td>\n      <td>18.788412</td>\n      <td>23.0</td>\n      <td>126</td>\n    </tr>\n    <tr>\n      <th>4735</th>\n      <td>22.0</td>\n      <td>987.2</td>\n      <td>1.3</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>1005.8</td>\n      <td>987.3</td>\n      <td>22.8</td>\n      <td>1007.2</td>\n      <td>1006.8</td>\n      <td>...</td>\n      <td>21.489285</td>\n      <td>22.266048</td>\n      <td>21.000013</td>\n      <td>20.479671</td>\n      <td>20.409552</td>\n      <td>20.125942</td>\n      <td>20.322477</td>\n      <td>18.719667</td>\n      <td>23.0</td>\n      <td>127</td>\n    </tr>\n    <tr>\n      <th>4736</th>\n      <td>21.7</td>\n      <td>987.3</td>\n      <td>2.5</td>\n      <td>2.2</td>\n      <td>0.0</td>\n      <td>1006.0</td>\n      <td>987.5</td>\n      <td>22.6</td>\n      <td>1007.4</td>\n      <td>1006.9</td>\n      <td>...</td>\n      <td>21.471090</td>\n      <td>22.217978</td>\n      <td>20.597713</td>\n      <td>20.390404</td>\n      <td>19.640066</td>\n      <td>20.054193</td>\n      <td>20.153816</td>\n      <td>18.622232</td>\n      <td>23.0</td>\n      <td>128</td>\n    </tr>\n    <tr>\n      <th>4737</th>\n      <td>21.6</td>\n      <td>987.5</td>\n      <td>1.8</td>\n      <td>2.6</td>\n      <td>0.0</td>\n      <td>1006.1</td>\n      <td>987.7</td>\n      <td>22.5</td>\n      <td>1007.6</td>\n      <td>1007.2</td>\n      <td>...</td>\n      <td>21.110674</td>\n      <td>21.274258</td>\n      <td>20.830555</td>\n      <td>20.303028</td>\n      <td>19.857246</td>\n      <td>19.893646</td>\n      <td>20.277950</td>\n      <td>18.283499</td>\n      <td>23.0</td>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>4738</th>\n      <td>21.5</td>\n      <td>987.7</td>\n      <td>2.0</td>\n      <td>2.4</td>\n      <td>0.0</td>\n      <td>1006.3</td>\n      <td>987.7</td>\n      <td>22.5</td>\n      <td>1007.6</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>21.197664</td>\n      <td>21.403278</td>\n      <td>20.986320</td>\n      <td>20.374630</td>\n      <td>19.837166</td>\n      <td>20.076649</td>\n      <td>20.027288</td>\n      <td>18.613560</td>\n      <td>23.0</td>\n      <td>130</td>\n    </tr>\n    <tr>\n      <th>4739</th>\n      <td>21.3</td>\n      <td>987.7</td>\n      <td>2.6</td>\n      <td>2.4</td>\n      <td>0.0</td>\n      <td>1006.4</td>\n      <td>987.7</td>\n      <td>22.4</td>\n      <td>1007.6</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>21.104771</td>\n      <td>20.797606</td>\n      <td>20.561764</td>\n      <td>20.272457</td>\n      <td>20.216244</td>\n      <td>19.767256</td>\n      <td>20.061575</td>\n      <td>19.214008</td>\n      <td>23.0</td>\n      <td>131</td>\n    </tr>\n    <tr>\n      <th>4740</th>\n      <td>21.1</td>\n      <td>987.7</td>\n      <td>1.4</td>\n      <td>2.7</td>\n      <td>0.0</td>\n      <td>1006.6</td>\n      <td>987.9</td>\n      <td>22.3</td>\n      <td>1007.8</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>20.888376</td>\n      <td>20.690214</td>\n      <td>20.658377</td>\n      <td>20.277390</td>\n      <td>19.997856</td>\n      <td>19.573341</td>\n      <td>19.843885</td>\n      <td>19.222441</td>\n      <td>23.0</td>\n      <td>132</td>\n    </tr>\n    <tr>\n      <th>4741</th>\n      <td>21.0</td>\n      <td>987.7</td>\n      <td>1.6</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1006.6</td>\n      <td>987.9</td>\n      <td>22.2</td>\n      <td>1007.8</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>20.711042</td>\n      <td>20.539783</td>\n      <td>20.613564</td>\n      <td>20.181747</td>\n      <td>19.734524</td>\n      <td>19.627962</td>\n      <td>19.680889</td>\n      <td>19.236675</td>\n      <td>22.5</td>\n      <td>133</td>\n    </tr>\n    <tr>\n      <th>4742</th>\n      <td>20.9</td>\n      <td>987.7</td>\n      <td>0.2</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1006.6</td>\n      <td>987.8</td>\n      <td>22.2</td>\n      <td>1007.7</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>20.316914</td>\n      <td>20.675463</td>\n      <td>20.586403</td>\n      <td>20.296913</td>\n      <td>19.537392</td>\n      <td>19.674313</td>\n      <td>19.456739</td>\n      <td>19.162935</td>\n      <td>22.5</td>\n      <td>134</td>\n    </tr>\n    <tr>\n      <th>4743</th>\n      <td>20.8</td>\n      <td>987.6</td>\n      <td>2.6</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1006.8</td>\n      <td>987.6</td>\n      <td>22.0</td>\n      <td>1007.3</td>\n      <td>1007.3</td>\n      <td>...</td>\n      <td>20.263968</td>\n      <td>20.372398</td>\n      <td>20.113317</td>\n      <td>20.311310</td>\n      <td>19.675192</td>\n      <td>19.656439</td>\n      <td>19.416220</td>\n      <td>18.821072</td>\n      <td>22.0</td>\n      <td>135</td>\n    </tr>\n    <tr>\n      <th>4744</th>\n      <td>20.5</td>\n      <td>987.7</td>\n      <td>1.1</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1006.8</td>\n      <td>987.8</td>\n      <td>21.9</td>\n      <td>1007.7</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>20.573229</td>\n      <td>20.376558</td>\n      <td>20.527258</td>\n      <td>19.958427</td>\n      <td>19.486250</td>\n      <td>19.645269</td>\n      <td>19.902588</td>\n      <td>18.955053</td>\n      <td>22.0</td>\n      <td>136</td>\n    </tr>\n    <tr>\n      <th>4745</th>\n      <td>20.2</td>\n      <td>987.8</td>\n      <td>1.1</td>\n      <td>1.6</td>\n      <td>0.0</td>\n      <td>1006.9</td>\n      <td>987.8</td>\n      <td>21.7</td>\n      <td>1007.7</td>\n      <td>1007.6</td>\n      <td>...</td>\n      <td>20.206909</td>\n      <td>19.749041</td>\n      <td>20.498755</td>\n      <td>19.816803</td>\n      <td>18.971029</td>\n      <td>19.155348</td>\n      <td>19.565237</td>\n      <td>18.668520</td>\n      <td>22.0</td>\n      <td>137</td>\n    </tr>\n    <tr>\n      <th>4746</th>\n      <td>20.1</td>\n      <td>987.9</td>\n      <td>0.3</td>\n      <td>1.1</td>\n      <td>0.0</td>\n      <td>1006.9</td>\n      <td>987.7</td>\n      <td>21.7</td>\n      <td>1007.4</td>\n      <td>1007.7</td>\n      <td>...</td>\n      <td>20.200026</td>\n      <td>19.740654</td>\n      <td>20.108601</td>\n      <td>19.909378</td>\n      <td>19.142656</td>\n      <td>19.213314</td>\n      <td>19.442717</td>\n      <td>18.589371</td>\n      <td>22.0</td>\n      <td>138</td>\n    </tr>\n    <tr>\n      <th>4747</th>\n      <td>19.9</td>\n      <td>987.6</td>\n      <td>0.9</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>1006.9</td>\n      <td>987.7</td>\n      <td>21.7</td>\n      <td>1007.5</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>20.014666</td>\n      <td>19.676689</td>\n      <td>20.255836</td>\n      <td>19.621681</td>\n      <td>19.100281</td>\n      <td>19.040098</td>\n      <td>19.484961</td>\n      <td>18.347324</td>\n      <td>21.5</td>\n      <td>139</td>\n    </tr>\n    <tr>\n      <th>4748</th>\n      <td>19.9</td>\n      <td>987.6</td>\n      <td>0.5</td>\n      <td>0.7</td>\n      <td>0.0</td>\n      <td>1006.8</td>\n      <td>987.7</td>\n      <td>21.6</td>\n      <td>1007.5</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>19.562740</td>\n      <td>19.893084</td>\n      <td>20.044662</td>\n      <td>19.696766</td>\n      <td>19.123640</td>\n      <td>19.247835</td>\n      <td>19.475515</td>\n      <td>18.564573</td>\n      <td>21.5</td>\n      <td>140</td>\n    </tr>\n    <tr>\n      <th>4749</th>\n      <td>19.7</td>\n      <td>987.7</td>\n      <td>0.9</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>1006.9</td>\n      <td>987.6</td>\n      <td>21.4</td>\n      <td>1007.4</td>\n      <td>1007.5</td>\n      <td>...</td>\n      <td>19.389511</td>\n      <td>19.311953</td>\n      <td>20.230379</td>\n      <td>19.535376</td>\n      <td>19.093515</td>\n      <td>19.028814</td>\n      <td>19.406361</td>\n      <td>18.389347</td>\n      <td>21.5</td>\n      <td>141</td>\n    </tr>\n    <tr>\n      <th>4750</th>\n      <td>19.4</td>\n      <td>987.7</td>\n      <td>0.9</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>1006.9</td>\n      <td>987.8</td>\n      <td>21.3</td>\n      <td>1007.6</td>\n      <td>1007.5</td>\n      <td>...</td>\n      <td>19.401882</td>\n      <td>19.472687</td>\n      <td>19.696304</td>\n      <td>19.418423</td>\n      <td>18.642723</td>\n      <td>18.986610</td>\n      <td>19.291885</td>\n      <td>18.357679</td>\n      <td>21.5</td>\n      <td>142</td>\n    </tr>\n    <tr>\n      <th>4751</th>\n      <td>19.1</td>\n      <td>987.6</td>\n      <td>1.0</td>\n      <td>0.3</td>\n      <td>0.0</td>\n      <td>1006.8</td>\n      <td>987.8</td>\n      <td>21.2</td>\n      <td>1007.5</td>\n      <td>1007.4</td>\n      <td>...</td>\n      <td>19.455137</td>\n      <td>19.701723</td>\n      <td>19.360321</td>\n      <td>19.589153</td>\n      <td>18.591835</td>\n      <td>18.894979</td>\n      <td>19.210253</td>\n      <td>18.406981</td>\n      <td>21.0</td>\n      <td>143</td>\n    </tr>\n  </tbody>\n</table>\n<p>432 rows × 60 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "train_y18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, x_val_new, y_train_new, y_val_new = train_test_split(train_y18[perm_col_list.iloc[:best_col_num].index], train_y18[target_col], random_state=1)\n",
    "\n",
    "print(x_train_new, x_val_new, y_train_new, y_val_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_cols = []\n",
    "for i in range(10, 30, 1):\n",
    "    eval_res = evaluate(xgbr, trainIsNull[perm_feat_imp.iloc[:i].index], trainIsNull['Y00'])\n",
    "    print(i, eval_res)\n",
    "    perm_cols.append((i, eval_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted = pd.DataFrame(perm_cols, columns=['col_num', 'score']).sort_values('score', ascending=False)\n",
    "col_num = sorted['col_num'].iloc[0]\n",
    "col_num\n",
    "## col_num = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res = evaluate(xgbr, trainIsNull[perm_feat_imp.iloc[:col_num].index], trainIsNull['Y00'])\n",
    "print(col_num, eval_res)"
   ]
  }
 ]
}