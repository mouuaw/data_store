{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8n7gos8pTdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec801f6f-cef1-4847-8437-81764f834053"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, MaxPooling2D, Conv2D, Reshape, Dropout, Flatten, AveragePooling2D, BatchNormalization, MaxPool2D, LeakyReLU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "submission = pd.read_csv('submission.csv')\n",
        "train_dataset = pd.read_csv('train.csv')\n",
        "test_dataset = pd.read_csv('test.csv')\n",
        "\n",
        "x_train, y_train = train_dataset[[(str(i)) for i in range(784)]].astype(np.float), tf.one_hot(train_dataset['digit'].values,10)\n",
        "x_train = x_train/255.0\n",
        "x_test = test_dataset[[(str(i)) for i in range(784)]].astype(np.float)\n",
        "x_test = x_test/255.0\n",
        "\n",
        "#픽셀값이 5 미만인 값을 0으로 처리\n",
        "#for i in range(784):\n",
        " #   train_dataset.loc[train_dataset[str(i)]<5, str(i)] = 0 #'0' ~ '784' 변수 까지 적용\n",
        "  #  test_dataset.loc[test_dataset[str(i)]<5, str(i)] = 0 #prediction에만 쓰는 데이터\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "        Reshape((28,28,1), input_shape=(784,)), #28,28,1로 shape을 바꿈\n",
        "        #conv-conv-maxpool-droput 층 반복         \n",
        "        Conv2D(32, (2, 2), activation='relu',padding='valid'),\n",
        "        Conv2D(32, (2, 2), activation='relu',padding='valid'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Conv2D(64, (2, 2), activation='relu',padding='same'),\n",
        "        Conv2D(64, (2, 2), activation='relu',padding='same'),\n",
        "        MaxPooling2D(2, 2),    \n",
        "        Dropout(0.3),    \n",
        "        \n",
        "        Conv2D(128, (2, 2), activation='relu',padding='same'),\n",
        "        Conv2D(128, (2, 2), activation='relu',padding='same'),\n",
        "        MaxPooling2D(pool_size = (2, 2)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Flatten(),\n",
        "        Dropout(0.5),\n",
        "        Dense(128, activation = 'relu'),\n",
        "        Dense(10, activation= 'softmax')\n",
        "    ])\n",
        "    \n",
        "optimizer=Adam(lr=0.0005)\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics = ['acc'])\n",
        "\n",
        "lowest_point = \"final_model.ckpt\"\n",
        "lowest_loss = ModelCheckpoint(filepath=lowest_point, \n",
        "                            save_weights_only=True, \n",
        "                            save_best_only=True, \n",
        "                            monitor='val_loss', \n",
        "                            verbose=1)\n",
        "\n",
        "hist = model.fit(x_train, y_train, epochs=150, validation_split=0.1, callbacks=[lowest_loss],batch_size=16,)\n",
        "model.load_weights(lowest_point) #최적모형 불러오기 \n",
        "\n",
        "aa= model.predict(x_test) #예측 \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 2.3012 - acc: 0.1238\n",
            "Epoch 00001: val_loss improved from inf to 2.29753, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 7ms/step - loss: 2.2999 - acc: 0.1248 - val_loss: 2.2975 - val_acc: 0.1317\n",
            "Epoch 2/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 2.1338 - acc: 0.2212\n",
            "Epoch 00002: val_loss improved from 2.29753 to 1.85737, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 2.1285 - acc: 0.2230 - val_loss: 1.8574 - val_acc: 0.3220\n",
            "Epoch 3/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 1.6801 - acc: 0.4242\n",
            "Epoch 00003: val_loss improved from 1.85737 to 1.21955, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 1.6719 - acc: 0.4281 - val_loss: 1.2196 - val_acc: 0.6341\n",
            "Epoch 4/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 1.3491 - acc: 0.5503\n",
            "Epoch 00004: val_loss improved from 1.21955 to 1.02660, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 1.3398 - acc: 0.5534 - val_loss: 1.0266 - val_acc: 0.6488\n",
            "Epoch 5/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 1.0887 - acc: 0.6401\n",
            "Epoch 00005: val_loss improved from 1.02660 to 0.87731, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 1.0778 - acc: 0.6435 - val_loss: 0.8773 - val_acc: 0.6829\n",
            "Epoch 6/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.9712 - acc: 0.6819\n",
            "Epoch 00006: val_loss improved from 0.87731 to 0.72454, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.9714 - acc: 0.6826 - val_loss: 0.7245 - val_acc: 0.7805\n",
            "Epoch 7/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.9082 - acc: 0.6892\n",
            "Epoch 00007: val_loss improved from 0.72454 to 0.62284, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.9004 - acc: 0.6918 - val_loss: 0.6228 - val_acc: 0.7610\n",
            "Epoch 8/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.7915 - acc: 0.7363\n",
            "Epoch 00008: val_loss improved from 0.62284 to 0.57848, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.7901 - acc: 0.7363 - val_loss: 0.5785 - val_acc: 0.8049\n",
            "Epoch 9/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.7212 - acc: 0.7563\n",
            "Epoch 00009: val_loss did not improve from 0.57848\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.7222 - acc: 0.7585 - val_loss: 0.6134 - val_acc: 0.7756\n",
            "Epoch 10/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.6885 - acc: 0.7610\n",
            "Epoch 00010: val_loss improved from 0.57848 to 0.55834, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.6857 - acc: 0.7613 - val_loss: 0.5583 - val_acc: 0.7805\n",
            "Epoch 11/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.6330 - acc: 0.7878\n",
            "Epoch 00011: val_loss did not improve from 0.55834\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.6394 - acc: 0.7862 - val_loss: 0.5961 - val_acc: 0.7805\n",
            "Epoch 12/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.6372 - acc: 0.7793\n",
            "Epoch 00012: val_loss improved from 0.55834 to 0.52556, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.6306 - acc: 0.7824 - val_loss: 0.5256 - val_acc: 0.8390\n",
            "Epoch 13/150\n",
            "105/116 [==========================>...] - ETA: 0s - loss: 0.5840 - acc: 0.7964\n",
            "Epoch 00013: val_loss improved from 0.52556 to 0.45870, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 0.5819 - acc: 0.7949 - val_loss: 0.4587 - val_acc: 0.8390\n",
            "Epoch 14/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.5342 - acc: 0.8165\n",
            "Epoch 00014: val_loss improved from 0.45870 to 0.40747, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.5279 - acc: 0.8182 - val_loss: 0.4075 - val_acc: 0.8780\n",
            "Epoch 15/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.5054 - acc: 0.8205\n",
            "Epoch 00015: val_loss did not improve from 0.40747\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.5080 - acc: 0.8188 - val_loss: 0.4371 - val_acc: 0.8585\n",
            "Epoch 16/150\n",
            "116/116 [==============================] - ETA: 0s - loss: 0.4873 - acc: 0.8313\n",
            "Epoch 00016: val_loss did not improve from 0.40747\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.4873 - acc: 0.8313 - val_loss: 0.4271 - val_acc: 0.8829\n",
            "Epoch 17/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.4887 - acc: 0.8298\n",
            "Epoch 00017: val_loss improved from 0.40747 to 0.39879, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.4895 - acc: 0.8296 - val_loss: 0.3988 - val_acc: 0.8780\n",
            "Epoch 18/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.4359 - acc: 0.8536\n",
            "Epoch 00018: val_loss improved from 0.39879 to 0.37295, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.4305 - acc: 0.8562 - val_loss: 0.3729 - val_acc: 0.8683\n",
            "Epoch 19/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.4231 - acc: 0.8559\n",
            "Epoch 00019: val_loss improved from 0.37295 to 0.36822, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.4261 - acc: 0.8546 - val_loss: 0.3682 - val_acc: 0.8780\n",
            "Epoch 20/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.4024 - acc: 0.8566\n",
            "Epoch 00020: val_loss did not improve from 0.36822\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.4044 - acc: 0.8557 - val_loss: 0.3828 - val_acc: 0.8634\n",
            "Epoch 21/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.3872 - acc: 0.8555\n",
            "Epoch 00021: val_loss improved from 0.36822 to 0.36451, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.3798 - acc: 0.8578 - val_loss: 0.3645 - val_acc: 0.8927\n",
            "Epoch 22/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.3719 - acc: 0.8620\n",
            "Epoch 00022: val_loss did not improve from 0.36451\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.3692 - acc: 0.8644 - val_loss: 0.3870 - val_acc: 0.8829\n",
            "Epoch 23/150\n",
            "106/116 [==========================>...] - ETA: 0s - loss: 0.3781 - acc: 0.8744\n",
            "Epoch 00023: val_loss did not improve from 0.36451\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.3760 - acc: 0.8752 - val_loss: 0.4697 - val_acc: 0.8634\n",
            "Epoch 24/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.3536 - acc: 0.8673\n",
            "Epoch 00024: val_loss did not improve from 0.36451\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.3564 - acc: 0.8681 - val_loss: 0.3747 - val_acc: 0.8927\n",
            "Epoch 25/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.3203 - acc: 0.8870\n",
            "Epoch 00025: val_loss did not improve from 0.36451\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.3176 - acc: 0.8877 - val_loss: 0.3947 - val_acc: 0.8829\n",
            "Epoch 26/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.3347 - acc: 0.8733\n",
            "Epoch 00026: val_loss did not improve from 0.36451\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.3464 - acc: 0.8709 - val_loss: 0.3991 - val_acc: 0.8683\n",
            "Epoch 27/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.3161 - acc: 0.8939\n",
            "Epoch 00027: val_loss did not improve from 0.36451\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.3188 - acc: 0.8920 - val_loss: 0.3804 - val_acc: 0.8780\n",
            "Epoch 28/150\n",
            "105/116 [==========================>...] - ETA: 0s - loss: 0.3264 - acc: 0.8881\n",
            "Epoch 00028: val_loss did not improve from 0.36451\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.3243 - acc: 0.8877 - val_loss: 0.3680 - val_acc: 0.8878\n",
            "Epoch 29/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.3208 - acc: 0.8874\n",
            "Epoch 00029: val_loss improved from 0.36451 to 0.34631, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.3220 - acc: 0.8850 - val_loss: 0.3463 - val_acc: 0.8878\n",
            "Epoch 30/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.2749 - acc: 0.8983\n",
            "Epoch 00030: val_loss did not improve from 0.34631\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.2835 - acc: 0.8958 - val_loss: 0.3533 - val_acc: 0.8927\n",
            "Epoch 31/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.2888 - acc: 0.8918\n",
            "Epoch 00031: val_loss did not improve from 0.34631\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.2876 - acc: 0.8915 - val_loss: 0.3793 - val_acc: 0.8634\n",
            "Epoch 32/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.2718 - acc: 0.8979\n",
            "Epoch 00032: val_loss improved from 0.34631 to 0.32970, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.2719 - acc: 0.8991 - val_loss: 0.3297 - val_acc: 0.8683\n",
            "Epoch 33/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.2578 - acc: 0.8970\n",
            "Epoch 00033: val_loss did not improve from 0.32970\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.2608 - acc: 0.8974 - val_loss: 0.3788 - val_acc: 0.8683\n",
            "Epoch 34/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.2687 - acc: 0.9005\n",
            "Epoch 00034: val_loss did not improve from 0.32970\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.2654 - acc: 0.9002 - val_loss: 0.3816 - val_acc: 0.8829\n",
            "Epoch 35/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.2718 - acc: 0.9019\n",
            "Epoch 00035: val_loss did not improve from 0.32970\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.2735 - acc: 0.9002 - val_loss: 0.3318 - val_acc: 0.8927\n",
            "Epoch 36/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.2176 - acc: 0.9167\n",
            "Epoch 00036: val_loss did not improve from 0.32970\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.2244 - acc: 0.9154 - val_loss: 0.3667 - val_acc: 0.8927\n",
            "Epoch 37/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.2512 - acc: 0.9136\n",
            "Epoch 00037: val_loss improved from 0.32970 to 0.32169, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.2461 - acc: 0.9159 - val_loss: 0.3217 - val_acc: 0.8829\n",
            "Epoch 38/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.2568 - acc: 0.9077\n",
            "Epoch 00038: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.2530 - acc: 0.9083 - val_loss: 0.3780 - val_acc: 0.8878\n",
            "Epoch 39/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.2127 - acc: 0.9245\n",
            "Epoch 00039: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.2094 - acc: 0.9257 - val_loss: 0.3767 - val_acc: 0.8829\n",
            "Epoch 40/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.2278 - acc: 0.9142\n",
            "Epoch 00040: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.2251 - acc: 0.9148 - val_loss: 0.3591 - val_acc: 0.8780\n",
            "Epoch 41/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.2169 - acc: 0.9273\n",
            "Epoch 00041: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.2240 - acc: 0.9251 - val_loss: 0.3877 - val_acc: 0.8829\n",
            "Epoch 42/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.1947 - acc: 0.9253\n",
            "Epoch 00042: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1990 - acc: 0.9251 - val_loss: 0.3670 - val_acc: 0.8927\n",
            "Epoch 43/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.1954 - acc: 0.9308\n",
            "Epoch 00043: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1949 - acc: 0.9305 - val_loss: 0.3398 - val_acc: 0.8878\n",
            "Epoch 44/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9292\n",
            "Epoch 00044: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1995 - acc: 0.9305 - val_loss: 0.3938 - val_acc: 0.8780\n",
            "Epoch 45/150\n",
            "116/116 [==============================] - ETA: 0s - loss: 0.2148 - acc: 0.9257\n",
            "Epoch 00045: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.2148 - acc: 0.9257 - val_loss: 0.3840 - val_acc: 0.8927\n",
            "Epoch 46/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.2045 - acc: 0.9289\n",
            "Epoch 00046: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.2006 - acc: 0.9305 - val_loss: 0.3378 - val_acc: 0.8780\n",
            "Epoch 47/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9275\n",
            "Epoch 00047: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1907 - acc: 0.9267 - val_loss: 0.3366 - val_acc: 0.8927\n",
            "Epoch 48/150\n",
            "107/116 [==========================>...] - ETA: 0s - loss: 0.1860 - acc: 0.9317\n",
            "Epoch 00048: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1889 - acc: 0.9311 - val_loss: 0.3243 - val_acc: 0.8976\n",
            "Epoch 49/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.1559 - acc: 0.9444\n",
            "Epoch 00049: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1569 - acc: 0.9441 - val_loss: 0.3473 - val_acc: 0.8829\n",
            "Epoch 50/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.1903 - acc: 0.9340\n",
            "Epoch 00050: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1939 - acc: 0.9327 - val_loss: 0.3348 - val_acc: 0.9122\n",
            "Epoch 51/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.1923 - acc: 0.9307\n",
            "Epoch 00051: val_loss did not improve from 0.32169\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1910 - acc: 0.9311 - val_loss: 0.3918 - val_acc: 0.8780\n",
            "Epoch 52/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.1676 - acc: 0.9398\n",
            "Epoch 00052: val_loss improved from 0.32169 to 0.27830, saving model to final_model.ckpt\n",
            "116/116 [==============================] - 1s 4ms/step - loss: 0.1762 - acc: 0.9387 - val_loss: 0.2783 - val_acc: 0.9024\n",
            "Epoch 53/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.1814 - acc: 0.9347\n",
            "Epoch 00053: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1824 - acc: 0.9338 - val_loss: 0.3327 - val_acc: 0.8976\n",
            "Epoch 54/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.1785 - acc: 0.9392\n",
            "Epoch 00054: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1747 - acc: 0.9403 - val_loss: 0.2927 - val_acc: 0.8976\n",
            "Epoch 55/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9501\n",
            "Epoch 00055: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1381 - acc: 0.9506 - val_loss: 0.3330 - val_acc: 0.9024\n",
            "Epoch 56/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.1570 - acc: 0.9478\n",
            "Epoch 00056: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1594 - acc: 0.9474 - val_loss: 0.3505 - val_acc: 0.8927\n",
            "Epoch 57/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.9413\n",
            "Epoch 00057: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1744 - acc: 0.9409 - val_loss: 0.3589 - val_acc: 0.8878\n",
            "Epoch 58/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.1469 - acc: 0.9459\n",
            "Epoch 00058: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1483 - acc: 0.9452 - val_loss: 0.3403 - val_acc: 0.8878\n",
            "Epoch 59/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.1933 - acc: 0.9297\n",
            "Epoch 00059: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1905 - acc: 0.9311 - val_loss: 0.3685 - val_acc: 0.8732\n",
            "Epoch 60/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.1364 - acc: 0.9561\n",
            "Epoch 00060: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1350 - acc: 0.9566 - val_loss: 0.3128 - val_acc: 0.9024\n",
            "Epoch 61/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.1564 - acc: 0.9494\n",
            "Epoch 00061: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1603 - acc: 0.9501 - val_loss: 0.3919 - val_acc: 0.8829\n",
            "Epoch 62/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.1508 - acc: 0.9460\n",
            "Epoch 00062: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1544 - acc: 0.9441 - val_loss: 0.3993 - val_acc: 0.8878\n",
            "Epoch 63/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.1377 - acc: 0.9499\n",
            "Epoch 00063: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1347 - acc: 0.9506 - val_loss: 0.3726 - val_acc: 0.9073\n",
            "Epoch 64/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9644\n",
            "Epoch 00064: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1117 - acc: 0.9642 - val_loss: 0.3812 - val_acc: 0.8976\n",
            "Epoch 65/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.1225 - acc: 0.9580\n",
            "Epoch 00065: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1233 - acc: 0.9577 - val_loss: 0.4696 - val_acc: 0.8829\n",
            "Epoch 66/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.1166 - acc: 0.9597\n",
            "Epoch 00066: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1148 - acc: 0.9598 - val_loss: 0.4893 - val_acc: 0.8829\n",
            "Epoch 67/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.1184 - acc: 0.9589\n",
            "Epoch 00067: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1198 - acc: 0.9588 - val_loss: 0.4875 - val_acc: 0.8829\n",
            "Epoch 68/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.9563\n",
            "Epoch 00068: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1364 - acc: 0.9555 - val_loss: 0.4362 - val_acc: 0.8927\n",
            "Epoch 69/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.1236 - acc: 0.9574\n",
            "Epoch 00069: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1242 - acc: 0.9571 - val_loss: 0.5053 - val_acc: 0.8585\n",
            "Epoch 70/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.1578 - acc: 0.9414\n",
            "Epoch 00070: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1531 - acc: 0.9436 - val_loss: 0.3156 - val_acc: 0.8927\n",
            "Epoch 71/150\n",
            "107/116 [==========================>...] - ETA: 0s - loss: 0.1547 - acc: 0.9527\n",
            "Epoch 00071: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1650 - acc: 0.9506 - val_loss: 0.2833 - val_acc: 0.9024\n",
            "Epoch 72/150\n",
            "116/116 [==============================] - ETA: 0s - loss: 0.1337 - acc: 0.9544\n",
            "Epoch 00072: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1337 - acc: 0.9544 - val_loss: 0.4218 - val_acc: 0.8829\n",
            "Epoch 73/150\n",
            "106/116 [==========================>...] - ETA: 0s - loss: 0.1126 - acc: 0.9581\n",
            "Epoch 00073: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1162 - acc: 0.9571 - val_loss: 0.4008 - val_acc: 0.8829\n",
            "Epoch 74/150\n",
            "115/116 [============================>.] - ETA: 0s - loss: 0.1286 - acc: 0.9543\n",
            "Epoch 00074: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1284 - acc: 0.9544 - val_loss: 0.4909 - val_acc: 0.8732\n",
            "Epoch 75/150\n",
            "107/116 [==========================>...] - ETA: 0s - loss: 0.1477 - acc: 0.9445\n",
            "Epoch 00075: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1471 - acc: 0.9452 - val_loss: 0.4540 - val_acc: 0.8780\n",
            "Epoch 76/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9668\n",
            "Epoch 00076: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1136 - acc: 0.9669 - val_loss: 0.5112 - val_acc: 0.8878\n",
            "Epoch 77/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.1143 - acc: 0.9608\n",
            "Epoch 00077: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1157 - acc: 0.9598 - val_loss: 0.4971 - val_acc: 0.8780\n",
            "Epoch 78/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.1130 - acc: 0.9615\n",
            "Epoch 00078: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1134 - acc: 0.9604 - val_loss: 0.4932 - val_acc: 0.8829\n",
            "Epoch 79/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.1189 - acc: 0.9639\n",
            "Epoch 00079: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1213 - acc: 0.9631 - val_loss: 0.4438 - val_acc: 0.8732\n",
            "Epoch 80/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.1058 - acc: 0.9626\n",
            "Epoch 00080: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1056 - acc: 0.9620 - val_loss: 0.4635 - val_acc: 0.8976\n",
            "Epoch 81/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.1053 - acc: 0.9647\n",
            "Epoch 00081: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1046 - acc: 0.9642 - val_loss: 0.3883 - val_acc: 0.8732\n",
            "Epoch 82/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.1172 - acc: 0.9634\n",
            "Epoch 00082: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1186 - acc: 0.9626 - val_loss: 0.4361 - val_acc: 0.8732\n",
            "Epoch 83/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.0858 - acc: 0.9702\n",
            "Epoch 00083: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0859 - acc: 0.9702 - val_loss: 0.4464 - val_acc: 0.8878\n",
            "Epoch 84/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.1037 - acc: 0.9640\n",
            "Epoch 00084: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1065 - acc: 0.9631 - val_loss: 0.3798 - val_acc: 0.8878\n",
            "Epoch 85/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9674\n",
            "Epoch 00085: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0918 - acc: 0.9674 - val_loss: 0.6302 - val_acc: 0.8683\n",
            "Epoch 86/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.1275 - acc: 0.9555\n",
            "Epoch 00086: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1255 - acc: 0.9566 - val_loss: 0.6016 - val_acc: 0.8585\n",
            "Epoch 87/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9403\n",
            "Epoch 00087: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1725 - acc: 0.9403 - val_loss: 0.4526 - val_acc: 0.8780\n",
            "Epoch 88/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.1235 - acc: 0.9536\n",
            "Epoch 00088: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1234 - acc: 0.9544 - val_loss: 0.4839 - val_acc: 0.8780\n",
            "Epoch 89/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9563\n",
            "Epoch 00089: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1252 - acc: 0.9560 - val_loss: 0.5416 - val_acc: 0.8634\n",
            "Epoch 90/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.1083 - acc: 0.9654\n",
            "Epoch 00090: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1076 - acc: 0.9653 - val_loss: 0.4384 - val_acc: 0.8976\n",
            "Epoch 91/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.1207 - acc: 0.9566\n",
            "Epoch 00091: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1214 - acc: 0.9571 - val_loss: 0.5253 - val_acc: 0.8634\n",
            "Epoch 92/150\n",
            "107/116 [==========================>...] - ETA: 0s - loss: 0.0869 - acc: 0.9725\n",
            "Epoch 00092: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0875 - acc: 0.9712 - val_loss: 0.4705 - val_acc: 0.8780\n",
            "Epoch 93/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.0980 - acc: 0.9637\n",
            "Epoch 00093: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0977 - acc: 0.9631 - val_loss: 0.4219 - val_acc: 0.8976\n",
            "Epoch 94/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.1057 - acc: 0.9648\n",
            "Epoch 00094: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1019 - acc: 0.9664 - val_loss: 0.4533 - val_acc: 0.8780\n",
            "Epoch 95/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.1202 - acc: 0.9595\n",
            "Epoch 00095: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1170 - acc: 0.9609 - val_loss: 0.4294 - val_acc: 0.8829\n",
            "Epoch 96/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.0985 - acc: 0.9679\n",
            "Epoch 00096: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0958 - acc: 0.9680 - val_loss: 0.5039 - val_acc: 0.8732\n",
            "Epoch 97/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.0897 - acc: 0.9699\n",
            "Epoch 00097: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0876 - acc: 0.9707 - val_loss: 0.5815 - val_acc: 0.8829\n",
            "Epoch 98/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.0911 - acc: 0.9679\n",
            "Epoch 00098: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0893 - acc: 0.9691 - val_loss: 0.5436 - val_acc: 0.8878\n",
            "Epoch 99/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.1051 - acc: 0.9682\n",
            "Epoch 00099: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1046 - acc: 0.9680 - val_loss: 0.5258 - val_acc: 0.8780\n",
            "Epoch 100/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.0900 - acc: 0.9718\n",
            "Epoch 00100: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0888 - acc: 0.9718 - val_loss: 0.5918 - val_acc: 0.8780\n",
            "Epoch 101/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9677\n",
            "Epoch 00101: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1067 - acc: 0.9680 - val_loss: 0.4664 - val_acc: 0.8732\n",
            "Epoch 102/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.0836 - acc: 0.9665\n",
            "Epoch 00102: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0823 - acc: 0.9674 - val_loss: 0.5650 - val_acc: 0.8732\n",
            "Epoch 103/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.1291 - acc: 0.9595\n",
            "Epoch 00103: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1286 - acc: 0.9593 - val_loss: 0.5038 - val_acc: 0.8732\n",
            "Epoch 104/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.0987 - acc: 0.9710\n",
            "Epoch 00104: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0988 - acc: 0.9707 - val_loss: 0.4863 - val_acc: 0.8732\n",
            "Epoch 105/150\n",
            "107/116 [==========================>...] - ETA: 0s - loss: 0.0750 - acc: 0.9755\n",
            "Epoch 00105: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0779 - acc: 0.9750 - val_loss: 0.5196 - val_acc: 0.8683\n",
            "Epoch 106/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.0889 - acc: 0.9657\n",
            "Epoch 00106: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0901 - acc: 0.9658 - val_loss: 0.6839 - val_acc: 0.8634\n",
            "Epoch 107/150\n",
            "106/116 [==========================>...] - ETA: 0s - loss: 0.1015 - acc: 0.9646\n",
            "Epoch 00107: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1008 - acc: 0.9653 - val_loss: 0.4678 - val_acc: 0.8829\n",
            "Epoch 108/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.1039 - acc: 0.9643\n",
            "Epoch 00108: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1044 - acc: 0.9647 - val_loss: 0.5062 - val_acc: 0.8829\n",
            "Epoch 109/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.1030 - acc: 0.9633\n",
            "Epoch 00109: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0990 - acc: 0.9647 - val_loss: 0.4833 - val_acc: 0.8878\n",
            "Epoch 110/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9803\n",
            "Epoch 00110: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0616 - acc: 0.9799 - val_loss: 0.6393 - val_acc: 0.8780\n",
            "Epoch 111/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.0776 - acc: 0.9722\n",
            "Epoch 00111: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0761 - acc: 0.9729 - val_loss: 0.5828 - val_acc: 0.8780\n",
            "Epoch 112/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.0886 - acc: 0.9701\n",
            "Epoch 00112: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0891 - acc: 0.9691 - val_loss: 0.5496 - val_acc: 0.8829\n",
            "Epoch 113/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.0892 - acc: 0.9710\n",
            "Epoch 00113: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0896 - acc: 0.9712 - val_loss: 0.5699 - val_acc: 0.8829\n",
            "Epoch 114/150\n",
            "106/116 [==========================>...] - ETA: 0s - loss: 0.1085 - acc: 0.9682\n",
            "Epoch 00114: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1113 - acc: 0.9674 - val_loss: 0.6270 - val_acc: 0.8927\n",
            "Epoch 115/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.1032 - acc: 0.9682\n",
            "Epoch 00115: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1052 - acc: 0.9664 - val_loss: 0.4929 - val_acc: 0.8780\n",
            "Epoch 116/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.0777 - acc: 0.9747\n",
            "Epoch 00116: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0796 - acc: 0.9729 - val_loss: 0.4403 - val_acc: 0.8878\n",
            "Epoch 117/150\n",
            "105/116 [==========================>...] - ETA: 0s - loss: 0.0724 - acc: 0.9720\n",
            "Epoch 00117: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0804 - acc: 0.9712 - val_loss: 0.5785 - val_acc: 0.8829\n",
            "Epoch 118/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.0832 - acc: 0.9708\n",
            "Epoch 00118: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0878 - acc: 0.9702 - val_loss: 0.4775 - val_acc: 0.9024\n",
            "Epoch 119/150\n",
            "105/116 [==========================>...] - ETA: 0s - loss: 0.1213 - acc: 0.9631\n",
            "Epoch 00119: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1175 - acc: 0.9636 - val_loss: 0.5598 - val_acc: 0.8780\n",
            "Epoch 120/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.0893 - acc: 0.9702\n",
            "Epoch 00120: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0873 - acc: 0.9707 - val_loss: 0.4515 - val_acc: 0.8829\n",
            "Epoch 121/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.0694 - acc: 0.9769\n",
            "Epoch 00121: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0686 - acc: 0.9772 - val_loss: 0.4949 - val_acc: 0.8634\n",
            "Epoch 122/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.0747 - acc: 0.9743\n",
            "Epoch 00122: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0755 - acc: 0.9740 - val_loss: 0.5026 - val_acc: 0.8878\n",
            "Epoch 123/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.0804 - acc: 0.9743\n",
            "Epoch 00123: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0808 - acc: 0.9745 - val_loss: 0.5276 - val_acc: 0.8927\n",
            "Epoch 124/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9655\n",
            "Epoch 00124: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1079 - acc: 0.9647 - val_loss: 0.4474 - val_acc: 0.9073\n",
            "Epoch 125/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9715\n",
            "Epoch 00125: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1010 - acc: 0.9712 - val_loss: 0.4404 - val_acc: 0.8976\n",
            "Epoch 126/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.0831 - acc: 0.9715\n",
            "Epoch 00126: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0809 - acc: 0.9723 - val_loss: 0.4219 - val_acc: 0.9024\n",
            "Epoch 127/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9731\n",
            "Epoch 00127: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0863 - acc: 0.9734 - val_loss: 0.5059 - val_acc: 0.8732\n",
            "Epoch 128/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.0829 - acc: 0.9771\n",
            "Epoch 00128: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0803 - acc: 0.9783 - val_loss: 0.4810 - val_acc: 0.8780\n",
            "Epoch 129/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.0902 - acc: 0.9685\n",
            "Epoch 00129: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0887 - acc: 0.9685 - val_loss: 0.4570 - val_acc: 0.8927\n",
            "Epoch 130/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.1164 - acc: 0.9641\n",
            "Epoch 00130: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1155 - acc: 0.9642 - val_loss: 0.5054 - val_acc: 0.8732\n",
            "Epoch 131/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.0876 - acc: 0.9688\n",
            "Epoch 00131: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0872 - acc: 0.9691 - val_loss: 0.4786 - val_acc: 0.8829\n",
            "Epoch 132/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.0879 - acc: 0.9705\n",
            "Epoch 00132: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0858 - acc: 0.9707 - val_loss: 0.5010 - val_acc: 0.8829\n",
            "Epoch 133/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9698\n",
            "Epoch 00133: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0791 - acc: 0.9696 - val_loss: 0.5270 - val_acc: 0.8585\n",
            "Epoch 134/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.0890 - acc: 0.9745\n",
            "Epoch 00134: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0876 - acc: 0.9745 - val_loss: 0.5311 - val_acc: 0.8780\n",
            "Epoch 135/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.0611 - acc: 0.9764\n",
            "Epoch 00135: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0596 - acc: 0.9772 - val_loss: 0.5487 - val_acc: 0.8829\n",
            "Epoch 136/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.0610 - acc: 0.9830\n",
            "Epoch 00136: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0618 - acc: 0.9832 - val_loss: 0.5708 - val_acc: 0.8829\n",
            "Epoch 137/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.0748 - acc: 0.9727\n",
            "Epoch 00137: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0781 - acc: 0.9718 - val_loss: 0.5987 - val_acc: 0.8878\n",
            "Epoch 138/150\n",
            "108/116 [==========================>...] - ETA: 0s - loss: 0.0713 - acc: 0.9751\n",
            "Epoch 00138: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0720 - acc: 0.9750 - val_loss: 0.5854 - val_acc: 0.8732\n",
            "Epoch 139/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9812\n",
            "Epoch 00139: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0633 - acc: 0.9810 - val_loss: 0.7100 - val_acc: 0.8878\n",
            "Epoch 140/150\n",
            "111/116 [===========================>..] - ETA: 0s - loss: 0.0758 - acc: 0.9764\n",
            "Epoch 00140: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0747 - acc: 0.9761 - val_loss: 0.5561 - val_acc: 0.8829\n",
            "Epoch 141/150\n",
            "110/116 [===========================>..] - ETA: 0s - loss: 0.0801 - acc: 0.9722\n",
            "Epoch 00141: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0788 - acc: 0.9723 - val_loss: 0.5535 - val_acc: 0.8976\n",
            "Epoch 142/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9751\n",
            "Epoch 00142: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0712 - acc: 0.9740 - val_loss: 0.7533 - val_acc: 0.8634\n",
            "Epoch 143/150\n",
            "113/116 [============================>.] - ETA: 0s - loss: 0.0772 - acc: 0.9729\n",
            "Epoch 00143: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0763 - acc: 0.9729 - val_loss: 0.6481 - val_acc: 0.8829\n",
            "Epoch 144/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.0901 - acc: 0.9720\n",
            "Epoch 00144: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0897 - acc: 0.9723 - val_loss: 0.5689 - val_acc: 0.8683\n",
            "Epoch 145/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.0504 - acc: 0.9851\n",
            "Epoch 00145: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0546 - acc: 0.9832 - val_loss: 0.5256 - val_acc: 0.8878\n",
            "Epoch 146/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.0837 - acc: 0.9749\n",
            "Epoch 00146: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0879 - acc: 0.9734 - val_loss: 0.5749 - val_acc: 0.8780\n",
            "Epoch 147/150\n",
            "109/116 [===========================>..] - ETA: 0s - loss: 0.0844 - acc: 0.9690\n",
            "Epoch 00147: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0823 - acc: 0.9696 - val_loss: 0.5888 - val_acc: 0.8878\n",
            "Epoch 148/150\n",
            "114/116 [============================>.] - ETA: 0s - loss: 0.1157 - acc: 0.9649\n",
            "Epoch 00148: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.1150 - acc: 0.9653 - val_loss: 0.5060 - val_acc: 0.8585\n",
            "Epoch 149/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.0947 - acc: 0.9676\n",
            "Epoch 00149: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0946 - acc: 0.9674 - val_loss: 0.6786 - val_acc: 0.8537\n",
            "Epoch 150/150\n",
            "112/116 [===========================>..] - ETA: 0s - loss: 0.0844 - acc: 0.9732\n",
            "Epoch 00150: val_loss did not improve from 0.27830\n",
            "116/116 [==============================] - 0s 4ms/step - loss: 0.0840 - acc: 0.9729 - val_loss: 0.6190 - val_acc: 0.8634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VZUXLALp1lM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "e5e4aa3e-b1d9-4656-bc1f-a06a08dfd601"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#loss, acc 그래프\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'black', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['acc'], 'black', label='train acc')\n",
        "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}